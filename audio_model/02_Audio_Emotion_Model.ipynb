{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyswarms scikit-fuzzy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:51:13.764120Z","iopub.execute_input":"2025-11-01T08:51:13.764647Z","iopub.status.idle":"2025-11-01T08:51:18.288870Z","shell.execute_reply.started":"2025-11-01T08:51:13.764622Z","shell.execute_reply":"2025-11-01T08:51:18.288097Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Collecting pyswarms\n  Downloading pyswarms-1.3.0-py2.py3-none-any.whl.metadata (33 kB)\nCollecting scikit-fuzzy\n  Downloading scikit_fuzzy-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pyswarms) (1.15.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pyswarms) (1.26.4)\nRequirement already satisfied: matplotlib>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from pyswarms) (3.7.2)\nRequirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from pyswarms) (25.3.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pyswarms) (4.67.1)\nRequirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyswarms) (1.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from pyswarms) (6.0.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.3.1->pyswarms) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.3.1->pyswarms) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.3.1->pyswarms) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.3.1->pyswarms) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.3.1->pyswarms) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.3.1->pyswarms) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.3.1->pyswarms) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.3.1->pyswarms) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->pyswarms) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->pyswarms) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->pyswarms) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->pyswarms) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->pyswarms) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->pyswarms) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=1.3.1->pyswarms) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pyswarms) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pyswarms) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->pyswarms) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->pyswarms) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->pyswarms) (2024.2.0)\nDownloading pyswarms-1.3.0-py2.py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scikit_fuzzy-0.5.0-py2.py3-none-any.whl (920 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.8/920.8 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-fuzzy, pyswarms\nSuccessfully installed pyswarms-1.3.0 scikit-fuzzy-0.5.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nrobust_imbalance_pipeline.py\n\nEnd-to-end pipeline to diagnose and aggressively fix minority-class detection problems\nfor audio-based stress/non-stress classification.\n\nAuthor: Generated for user (modular, commented, ready-to-run).\n\"\"\"\n\nimport os\nimport random\nimport shutil\nimport math\nimport json\nfrom pathlib import Path\nfrom typing import Tuple, List, Dict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport librosa\nimport librosa.display\nimport soundfile as sf\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, backend as K\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    precision_recall_fscore_support, confusion_matrix\n)\nfrom sklearn.utils import class_weight\n\n# --------------------------\n# CUDA Configuration\n# --------------------------\n# Configure GPU memory growth to avoid OOM errors\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"[CUDA] Found {len(gpus)} GPU(s). Enabled memory growth.\")\n        # Set visible devices if needed\n        # tf.config.set_visible_devices(gpus[0], 'GPU')  # Use only first GPU\n    except RuntimeError as e:\n        print(f\"[CUDA] Error configuring GPU: {e}\")\nelse:\n    print(\"[CUDA] No GPU found, using CPU.\")\n\n# Set CUDA devices (optional - uncomment if you want to specify which GPU to use)\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use GPU 0\n\n# --------------------------\n# Configuration / Constants\n# --------------------------\nSEED = 1337\nnp.random.seed(SEED)\nrandom.seed(SEED)\ntf.random.set_seed(SEED)\n\n# Paths - adjust base input path as required\nINPUT_AUDIO_DIR = \"/kaggle/input/cremad/AudioWAV\"  # change if needed\nBASE_DIR = \"./BASE_DIR\"\nMODEL_DIR = os.path.join(BASE_DIR, \"models\", \"robust_models\")\nANALYSIS_DIR = os.path.join(BASE_DIR, \"imbalance_analysis\")\nVIZ_DIR = os.path.join(BASE_DIR, \"visualizations\", \"model_comparisons\")\nos.makedirs(MODEL_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)\nos.makedirs(VIZ_DIR, exist_ok=True)\n\nSAMPLE_RATE = 16000\nMAX_DURATION = 3.0  # seconds: clips will be padded/truncated to this duration\nMAX_SAMPLES = int(SAMPLE_RATE * MAX_DURATION)\nN_MFCC = 40\n\n# Training params\nBATCH_SIZE = 32\nEPOCHS = 30\nLEARNING_RATE = 1e-3\nVALIDATION_SPLIT = 0.15\nTEST_SPLIT = 0.15\n\nTHRESH_SEARCH_EVERY = 2  # epochs\nTHRESH_SEARCH_STEPS = 101  # how many thresholds to search between 0 and 1\n\n# Aggressive augmentation multipliers\nMINORITY_AUG_FACTOR = 3  # how many augmented versions to create per minority sample\n\n# Stress mapping\nSTRESS_CODES = {\"ANG\", \"DIS\", \"FEA\", \"SAD\"}\nNON_STRESS_CODES = {\"HAP\", \"NEU\"}\n\n# --------------------------\n# Utility Functions\n# --------------------------\n\ndef safe_mkdir(p):\n    os.makedirs(p, exist_ok=True)\n\ndef list_wav_files(directory: str) -> List[str]:\n    files = [f for f in os.listdir(directory) if f.lower().endswith(\".wav\")]\n    return files\n\ndef parse_emotion_from_filename(fname: str) -> str:\n    parts = fname.split('_')\n    if len(parts) >= 3:\n        return parts[2].upper()\n    return \"UNK\"\n\ndef map_to_binary_label(emotion_code: str) -> int:\n    if emotion_code in STRESS_CODES:\n        return 1\n    elif emotion_code in NON_STRESS_CODES:\n        return 0\n    else:\n        # treat unknown as non-stress (or choose to skip)\n        return 0\n\ndef load_audio(path: str, sr: int = SAMPLE_RATE, max_samples: int = MAX_SAMPLES) -> np.ndarray:\n    y, _sr = librosa.load(path, sr=sr)\n    # pad or truncate to max_samples\n    if len(y) < max_samples:\n        y = np.pad(y, (0, max_samples - len(y)))\n    else:\n        y = y[:max_samples]\n    return y\n\ndef extract_mfcc(y: np.ndarray, sr: int = SAMPLE_RATE, n_mfcc: int = N_MFCC) -> np.ndarray:\n    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n    # normalize per-sample\n    mfcc = (mfcc - np.mean(mfcc)) / (np.std(mfcc) + 1e-12)\n    return mfcc.astype(np.float32)  # shape: (n_mfcc, time_frames)\n\ndef feature_to_model_input(feat: np.ndarray) -> np.ndarray:\n    # Keras Conv1D expects (time_steps, channels). We'll transpose MFCC to (time, n_mfcc)\n    arr = feat.T  # shape: (time_frames, n_mfcc)\n    return arr\n\n# --------------------------\n# Aggressive Augmentation\n# --------------------------\n\ndef add_white_noise(y: np.ndarray, snr_db: float = 20.0) -> np.ndarray:\n    # Additive white noise at desired SNR (dB)\n    rms_signal = np.sqrt(np.mean(y**2))\n    snr = 10 ** (snr_db / 20.0)\n    rms_noise = rms_signal / snr\n    noise = np.random.normal(0, rms_noise, size=y.shape)\n    return y + noise\n\ndef time_shift(y: np.ndarray, shift_max: float = 0.2) -> np.ndarray:\n    # shift fraction of total length\n    shift = int(random.uniform(-shift_max, shift_max) * len(y))\n    return np.roll(y, shift)\n\ndef pitch_shift(y: np.ndarray, sr: int = SAMPLE_RATE, n_steps: float = None) -> np.ndarray:\n    if n_steps is None:\n        n_steps = random.uniform(-2.0, 2.0)\n    try:\n        return librosa.effects.pitch_shift(y, sr=sr, n_steps=n_steps)\n    except Exception:\n        return y\n\ndef time_stretch(y: np.ndarray, rate: float = None) -> np.ndarray:\n    if rate is None:\n        rate = random.uniform(0.9, 1.1)\n    try:\n        y_st = librosa.effects.time_stretch(y, rate)\n        # pad/truncate to original length\n        if len(y_st) < len(y):\n            y_st = np.pad(y_st, (0, len(y) - len(y_st)))\n        else:\n            y_st = y_st[:len(y)]\n        return y_st\n    except Exception:\n        return y\n\ndef spec_augment(mfcc: np.ndarray, freq_mask_param: int = 8, time_mask_param: int = 10) -> np.ndarray:\n    # Simple mask on MFCCs (works on mfcc features)\n    mf = mfcc.copy()\n    num_mel_channels = mf.shape[0]\n    num_time_steps = mf.shape[1]\n    # freq mask\n    f = random.randrange(0, freq_mask_param + 1)\n    f0 = random.randrange(0, max(1, num_mel_channels - f))\n    mf[f0:f0 + f, :] = 0\n    # time mask\n    t = random.randrange(0, time_mask_param + 1)\n    t0 = random.randrange(0, max(1, num_time_steps - t))\n    mf[:, t0:t0 + t] = 0\n    return mf\n\ndef augment_audio_aggressive(y: np.ndarray, sr: int = SAMPLE_RATE) -> List[np.ndarray]:\n    \"\"\"Return multiple aggressively augmented versions of waveform y.\"\"\"\n    out = []\n    # variant 1: noise + shift\n    out.append(add_white_noise(time_shift(y), snr_db=random.uniform(8, 18)))\n    # variant 2: pitch shift + stretch\n    out.append(pitch_shift(time_stretch(y, rate=random.uniform(0.85, 1.15)), sr=sr, n_steps=random.uniform(-3, 3)))\n    # variant 3: noise + pitch\n    out.append(add_white_noise(pitch_shift(y, sr=sr, n_steps=random.uniform(-2, 2)), snr_db=random.uniform(10, 24)))\n    # optionally add original slightly modified\n    out.append(time_stretch(y, rate=random.uniform(0.95, 1.05)))\n    return out\n\ndef augment_audio_features_aggressive(X_feats: List[np.ndarray], y_waveforms: List[np.ndarray], labels: List[int],\n                                     minority_label: int = 1, factor: int = MINORITY_AUG_FACTOR):\n    \"\"\"\n    Aggressively augment minority class: for each minority sample, create 'factor' augmented instances,\n    return new feature arrays and labels appended.\n    X_feats: list of MFCC ndarrays (n_mfcc, time)\n    y_waveforms: list of raw waveforms corresponding to X_feats (same order)\n    labels: list of ints\n    \"\"\"\n    print(\"[augment_audio_features_aggressive] Start aggressive augmentation for minority class...\")\n    new_feats = []\n    new_labels = []\n    for mfcc, wav, label in zip(X_feats, y_waveforms, labels):\n        if label != minority_label:\n            continue\n        augmented_waves = augment_audio_aggressive(wav)\n        for aw in augmented_waves[:factor]:  # limit factor\n            # extract mfcc for augmented waveform\n            am = extract_mfcc(aw)\n            # apply spec augment too for variety\n            am = spec_augment(am, freq_mask_param=10, time_mask_param=15)\n            new_feats.append(am)\n            new_labels.append(label)\n    print(f\"[augment_audio_features_aggressive] Created {len(new_feats)} augmented minority features.\")\n    return new_feats, new_labels\n\n# --------------------------\n# Diagnosis & Utility\n# --------------------------\n\ndef print_class_distribution(y: np.ndarray, prefix: str = \"\"):\n    unique, counts = np.unique(y, return_counts=True)\n    d = dict(zip(unique, counts))\n    print(f\"\\n[{prefix}] Class distribution:\")\n    for k in sorted(d.keys()):\n        print(f\"  Class {k}: {d[k]} samples\")\n    maj = max(d.values())\n    minv = min(d.values())\n    ratio = maj / (minv + 1e-12)\n    print(f\"  Imbalance ratio (maj/min): {ratio:.3f}\")\n    return d, ratio\n\ndef compute_class_weights(y: np.ndarray, method: str = \"inverse_freq\"):\n    # returns dict {0: w0, 1: w1}\n    vals = np.unique(y)\n    if method == \"inverse_freq\":\n        weights = class_weight.compute_class_weight('balanced', classes=vals, y=y)\n        return {int(v): float(w) for v, w in zip(vals, weights)}\n    elif method == \"sqrt_inv\":\n        unique, counts = np.unique(y, return_counts=True)\n        inv = {u: 1.0 / math.sqrt(c) for u, c in zip(unique, counts)}\n        # normalize to sum=2 (so average=1)\n        s = sum(inv.values())\n        norm = {u: (inv[u] * len(inv)) / s for u in inv}\n        return {int(u): float(norm[u]) for u in norm}\n    else:\n        raise ValueError(\"Unknown method\")\n\n# --------------------------\n# Losses: Dynamic Focal Loss\n# --------------------------\n\ndef dynamic_focal_loss(alpha_pos: float = 0.75, alpha_neg: float = None, gamma: float = 2.0):\n    \"\"\"\n    Returns a loss function that computes focal loss and allows dynamic alpha for positive/negative.\n    alpha_pos: weight for positive class (minority). If alpha_neg is None, set to 1-alpha_pos.\n    gamma: focusing parameter.\n    \"\"\"\n    if alpha_neg is None:\n        alpha_neg = 1.0 - alpha_pos\n\n    def loss(y_true, y_pred):\n        y_true = K.cast(y_true, K.floatx())\n        # clip\n        eps = K.epsilon()\n        y_pred = K.clip(y_pred, eps, 1.0 - eps)\n        # focal\n        pt = tf.where(K.equal(y_true, 1), y_pred, 1 - y_pred)\n        alpha = tf.where(K.equal(y_true, 1), alpha_pos, alpha_neg)\n        loss_tensor = -alpha * K.pow(1.0 - pt, gamma) * K.log(pt)\n        return K.mean(loss_tensor)\n    return loss\n\n# --------------------------\n# MODEL DEFINITIONS - 5 DIFFERENT ARCHITECTURES\n# --------------------------\n\ndef create_cnn_bilstm_model(input_shape: Tuple[int, int], dropout_rate: float = 0.3) -> tf.keras.Model:\n    \"\"\"Model 1: CNN + BiLSTM (Original robust model)\"\"\"\n    inp = layers.Input(shape=input_shape, name=\"input_mfcc\")\n    x = inp\n\n    # Conv blocks\n    for i, (filters, kernel) in enumerate([(64, 3), (128, 3), (256, 3)]):\n        x = layers.Conv1D(filters=filters, kernel_size=kernel, padding=\"same\", activation=\"relu\", name=f\"conv_{i}\")(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.MaxPool1D(pool_size=2)(x)\n        x = layers.Dropout(dropout_rate)(x)\n\n    # BiLSTM layers\n    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n    x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(x)\n\n    x = layers.Dense(128, activation=\"relu\")(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.Dense(64, activation=\"relu\")(x)\n    x = layers.Dropout(dropout_rate)(x)\n\n    out = layers.Dense(1, activation=\"sigmoid\", name=\"out\")(x)\n    model = models.Model(inputs=inp, outputs=out)\n    return model\n\ndef create_cnn_only_model(input_shape: Tuple[int, int], dropout_rate: float = 0.3) -> tf.keras.Model:\n    \"\"\"Model 2: Pure CNN architecture (no RNN layers)\"\"\"\n    inp = layers.Input(shape=input_shape, name=\"input_mfcc\")\n    x = inp\n\n    # Deeper CNN architecture\n    for i, (filters, kernel) in enumerate([(64, 5), (128, 5), (256, 3), (512, 3)]):\n        x = layers.Conv1D(filters=filters, kernel_size=kernel, padding=\"same\", activation=\"relu\")(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.MaxPool1D(pool_size=2)(x)\n        x = layers.Dropout(dropout_rate)(x)\n\n    # Global pooling instead of RNN\n    x = layers.GlobalAveragePooling1D()(x)\n    \n    # Dense layers\n    x = layers.Dense(256, activation=\"relu\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(dropout_rate)(x)\n    \n    x = layers.Dense(128, activation=\"relu\")(x)\n    x = layers.Dropout(dropout_rate)(x)\n    \n    x = layers.Dense(64, activation=\"relu\")(x)\n    x = layers.Dropout(dropout_rate)(x)\n\n    out = layers.Dense(1, activation=\"sigmoid\", name=\"out\")(x)\n    model = models.Model(inputs=inp, outputs=out)\n    return model\n\ndef create_lstm_model(input_shape: Tuple[int, int], dropout_rate: float = 0.3) -> tf.keras.Model:\n    \"\"\"Model 3: Pure LSTM model (no CNN layers)\"\"\"\n    inp = layers.Input(shape=input_shape, name=\"input_mfcc\")\n    x = inp\n\n    # Multiple LSTM layers\n    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(dropout_rate)(x)\n    \n    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(dropout_rate)(x)\n    \n    x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(dropout_rate)(x)\n\n    # Dense layers\n    x = layers.Dense(128, activation=\"relu\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(dropout_rate)(x)\n    \n    x = layers.Dense(64, activation=\"relu\")(x)\n    x = layers.Dropout(dropout_rate)(x)\n\n    out = layers.Dense(1, activation=\"sigmoid\", name=\"out\")(x)\n    model = models.Model(inputs=inp, outputs=out)\n    return model\n\ndef create_lightweight_transformer(input_shape: Tuple[int, int], dropout_rate: float = 0.3) -> tf.keras.Model:\n    \"\"\"Model 4: Lightweight Transformer architecture\"\"\"\n    inp = layers.Input(shape=input_shape, name=\"input_mfcc\")\n    \n    # Positional encoding (simplified)\n    x = layers.LayerNormalization(epsilon=1e-6)(inp)\n    \n    # Multi-head attention blocks\n    for i in range(2):  # 2 transformer blocks\n        # Self-attention\n        attention_output = layers.MultiHeadAttention(\n            num_heads=4, \n            key_dim=64, \n            dropout=dropout_rate\n        )(x, x)\n        \n        x = layers.Add()([x, attention_output])\n        x = layers.LayerNormalization(epsilon=1e-6)(x)\n        \n        # Feed-forward network\n        ffn = models.Sequential([\n            layers.Dense(128, activation=\"relu\"),\n            layers.Dense(input_shape[-1]),\n        ])\n        ffn_output = ffn(x)\n        \n        x = layers.Add()([x, ffn_output])\n        x = layers.LayerNormalization(epsilon=1e-6)(x)\n    \n    # Global pooling and classification\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(dropout_rate)(x)\n    \n    x = layers.Dense(64, activation=\"relu\")(x)\n    x = layers.Dropout(dropout_rate)(x)\n    \n    out = layers.Dense(1, activation=\"sigmoid\", name=\"out\")(x)\n    model = models.Model(inputs=inp, outputs=out)\n    return model\n\ndef create_hybrid_cnn_lstm_model(input_shape: Tuple[int, int], dropout_rate: float = 0.3) -> tf.keras.Model:\n    \"\"\"Model 5: Hybrid CNN-LSTM with parallel branches\"\"\"\n    inp = layers.Input(shape=input_shape, name=\"input_mfcc\")\n    \n    # Branch 1: CNN path\n    cnn_branch = layers.Conv1D(64, 5, activation='relu', padding='same')(inp)\n    cnn_branch = layers.BatchNormalization()(cnn_branch)\n    cnn_branch = layers.MaxPool1D(2)(cnn_branch)\n    cnn_branch = layers.Dropout(dropout_rate)(cnn_branch)\n    \n    cnn_branch = layers.Conv1D(128, 3, activation='relu', padding='same')(cnn_branch)\n    cnn_branch = layers.BatchNormalization()(cnn_branch)\n    cnn_branch = layers.MaxPool1D(2)(cnn_branch)\n    cnn_branch = layers.Dropout(dropout_rate)(cnn_branch)\n    \n    cnn_branch = layers.GlobalAveragePooling1D()(cnn_branch)\n    \n    # Branch 2: LSTM path\n    lstm_branch = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(inp)\n    lstm_branch = layers.BatchNormalization()(lstm_branch)\n    lstm_branch = layers.Dropout(dropout_rate)(lstm_branch)\n    \n    lstm_branch = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(lstm_branch)\n    lstm_branch = layers.BatchNormalization()(lstm_branch)\n    lstm_branch = layers.Dropout(dropout_rate)(lstm_branch)\n    \n    # Concatenate both branches\n    combined = layers.Concatenate()([cnn_branch, lstm_branch])\n    \n    # Dense layers\n    x = layers.Dense(256, activation=\"relu\")(combined)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(dropout_rate)(x)\n    \n    x = layers.Dense(128, activation=\"relu\")(x)\n    x = layers.Dropout(dropout_rate)(x)\n    \n    x = layers.Dense(64, activation=\"relu\")(x)\n    x = layers.Dropout(dropout_rate)(x)\n\n    out = layers.Dense(1, activation=\"sigmoid\", name=\"out\")(x)\n    model = models.Model(inputs=inp, outputs=out)\n    return model\n\n# --------------------------\n# Threshold Optimizer Callback\n# --------------------------\n\nclass ThresholdOptimizer(Callback):\n    \"\"\"\n    Every 'every_n_epochs' epochs, compute validation predictions and find threshold that maximizes\n    minority class F1 (positive label).\n    Save best threshold to file and keep a callback state.\n    \"\"\"\n    def __init__(self, validation_data, every_n_epochs=THRESH_SEARCH_EVERY, steps=THRESH_SEARCH_STEPS, save_path=None):\n        super().__init__()\n        self.x_val, self.y_val = validation_data\n        self.every = every_n_epochs\n        self.steps = steps\n        self.best_threshold = 0.5\n        self.best_f1 = -1.0\n        self.save_path = save_path\n\n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch + 1) % self.every != 0:\n            return\n        print(f\"[ThresholdOptimizer] Searching threshold at epoch {epoch+1} ...\")\n        y_proba = self.model.predict(self.x_val, batch_size=64, verbose=0).ravel()\n        thresholds = np.linspace(0.0, 1.0, self.steps)\n        for t in thresholds:\n            y_pred = (y_proba >= t).astype(int)\n            f1 = f1_score(self.y_val, y_pred, pos_label=1)\n            if f1 > self.best_f1:\n                self.best_f1 = f1\n                self.best_threshold = float(t)\n        print(f\"[ThresholdOptimizer] Best threshold so far: {self.best_threshold:.3f} with minority F1={self.best_f1:.4f}\")\n        # persist\n        if self.save_path:\n            with open(self.save_path, \"w\") as f:\n                json.dump({\"best_threshold\": self.best_threshold, \"best_f1\": self.best_f1}, f)\n\n# --------------------------\n# Training & Evaluation Helpers\n# --------------------------\n\ndef prepare_dataset_from_directory(audio_dir: str):\n    print(\"[prepare_dataset_from_directory] Scanning audio directory and extracting features...\")\n    files = list_wav_files(audio_dir)\n    records = []\n    X_feats = []\n    X_waveforms = []\n    labels = []\n    filenames = []\n\n    for f in files:\n        emotion = parse_emotion_from_filename(f)\n        label = map_to_binary_label(emotion)\n        p = os.path.join(audio_dir, f)\n        try:\n            wav = load_audio(p)\n            mfcc = extract_mfcc(wav)\n            X_feats.append(mfcc)\n            X_waveforms.append(wav)\n            labels.append(label)\n            filenames.append(f)\n        except Exception as e:\n            print(f\"  Warning: failed to load {p}: {e}\")\n    print(f\"[prepare_dataset_from_directory] Loaded {len(X_feats)} audio files.\")\n    return X_feats, X_waveforms, np.array(labels), filenames\n\ndef pad_or_trim_features(feat_list: List[np.ndarray], target_frames: int = None) -> np.ndarray:\n    # All MFCCs may have different time_frames; pad/truncate along time axis to uniform length\n    lengths = [f.shape[1] for f in feat_list]\n    if target_frames is None:\n        target_frames = int(np.percentile(lengths, 90))\n        target_frames = max(10, target_frames)\n    out = []\n    for f in feat_list:\n        n_mfcc = f.shape[0]\n        frames = f.shape[1]\n        if frames < target_frames:\n            pad_width = target_frames - frames\n            f_p = np.pad(f, ((0,0), (0, pad_width)), mode='constant')\n        else:\n            f_p = f[:, :target_frames]\n        out.append(f_p)\n    # Convert to shape (N, time, channels)\n    arr = np.stack([feature_to_model_input(f) for f in out], axis=0)\n    return arr  # shape: (N, time_steps, n_mfcc)\n\ndef evaluate_at_threshold(y_true: np.ndarray, y_proba: np.ndarray, threshold: float):\n    y_pred = (y_proba >= threshold).astype(int)\n    acc = accuracy_score(y_true, y_pred)\n    prec = precision_score(y_true, y_pred, zero_division=0)\n    rec = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    # minority metrics (positive label = 1)\n    prec_pos = precision_score(y_true, y_pred, pos_label=1, zero_division=0)\n    rec_pos = recall_score(y_true, y_pred, pos_label=1, zero_division=0)\n    f1_pos = f1_score(y_true, y_pred, pos_label=1, zero_division=0)\n    return {\n        \"accuracy\": acc,\n        \"precision\": prec,\n        \"recall\": rec,\n        \"f1\": f1,\n        \"minority_precision\": prec_pos,\n        \"minority_recall\": rec_pos,\n        \"minority_f1\": f1_pos\n    }\n\n# --------------------------\n# Model Training Function\n# --------------------------\n\ndef train_and_evaluate_model(X_feats, y, model_name, model_creator, aggressive=False):\n    \"\"\"\n    Train a specific model with given architecture.\n    \"\"\"\n    print(f\"\\n[train_and_evaluate_model] Starting training for '{model_name}' (aggressive={aggressive})\")\n    \n    # split into train+val+test stratified\n    X_temp, X_test, y_temp, y_test = train_test_split(X_feats, y, test_size=TEST_SPLIT, stratify=y, random_state=SEED)\n    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=VALIDATION_SPLIT/(1-TEST_SPLIT), stratify=y_temp, random_state=SEED)\n\n    print_class_distribution(y_train, prefix=f\"Train-{model_name}\")\n    print_class_distribution(y_val, prefix=f\"Val-{model_name}\")\n    print_class_distribution(y_test, prefix=f\"Test-{model_name}\")\n\n    # Compute target frames and prepare data\n    target_frames = int(np.percentile([mf.shape[1] for mf in X_train], 90))\n    target_frames = max(10, target_frames)\n    print(f\"[train_and_evaluate_model] Target time frames for model input: {target_frames}\")\n\n    def stack_data(list_feats):\n        return pad_or_trim_features(list_feats, target_frames)\n\n    X_train_arr = stack_data(X_train)\n    X_val_arr = stack_data(X_val)\n    X_test_arr = stack_data(X_test)\n\n    # Apply aggressive augmentation if requested\n    if aggressive:\n        print(f\"[train_and_evaluate_model] Applying heavy spec_augment to minority samples for {model_name}...\")\n        augmented_feats = []\n        augmented_labels = []\n        for mf, label in zip(X_train, y_train):\n            if label == 1:\n                for _ in range(MINORITY_AUG_FACTOR):\n                    new_m = spec_augment(mf, freq_mask_param=12, time_mask_param=20)\n                    augmented_feats.append(new_m)\n                    augmented_labels.append(label)\n        print(f\"[train_and_evaluate_model] Created {len(augmented_feats)} augmented MFCC samples for minority.\")\n        X_train_ext = X_train + augmented_feats\n        y_train_ext = np.concatenate([y_train, np.array(augmented_labels)]) if len(augmented_labels) > 0 else y_train\n        X_train_arr = stack_data(X_train_ext)\n        y_train = y_train_ext\n        print(f\"[train_and_evaluate_model] New training size: {X_train_arr.shape[0]}\")\n\n    # compute class weights for training\n    cw = compute_class_weights(y_train, method=\"inverse_freq\")\n    print(f\"[train_and_evaluate_model] Class weights used for training: {cw}\")\n\n    # Prepare model\n    input_shape = (X_train_arr.shape[1], X_train_arr.shape[2])\n    model = model_creator(input_shape)\n    \n    # Choose loss function\n    if aggressive:\n        alpha_pos = cw.get(1, 1.0) / (cw.get(1, 1.0) + cw.get(0, 1.0))\n        alpha_pos = min(0.95, max(0.5, float(alpha_pos)))\n        loss_fn = dynamic_focal_loss(alpha_pos=alpha_pos, gamma=2.0)\n        print(f\"[train_and_evaluate_model] Using dynamic focal loss with alpha_pos={alpha_pos:.4f}\")\n    else:\n        loss_fn = tf.keras.losses.BinaryCrossentropy()\n        print(\"[train_and_evaluate_model] Using BinaryCrossentropy loss (baseline).\")\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n    model.compile(optimizer=optimizer, loss=loss_fn, \n                  metrics=[tf.keras.metrics.BinaryAccuracy(), \n                          tf.keras.metrics.AUC(name=\"auc\"),\n                          tf.keras.metrics.Precision(name=\"precision\"),\n                          tf.keras.metrics.Recall(name=\"recall\")])\n    \n    print(f\"\\n[Model Architecture - {model_name}]\")\n    model.summary()\n\n    # Callbacks\n    model_filename = f\"model_{model_name}_{'robust' if aggressive else 'baseline'}.h5\"\n    model_path = os.path.join(MODEL_DIR, model_filename)\n    checkpoint = ModelCheckpoint(model_path, monitor=\"val_loss\", save_best_only=True, verbose=1)\n    early = EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True, verbose=1)\n    reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n\n    # Threshold optimizer\n    thresh_path = os.path.join(ANALYSIS_DIR, f\"threshold_{model_name}_{'robust' if aggressive else 'baseline'}.json\")\n    thresh_cb = ThresholdOptimizer(validation_data=(X_val_arr, y_val), every_n_epochs=THRESH_SEARCH_EVERY, steps=THRESH_SEARCH_STEPS, save_path=thresh_path)\n\n    print(f\"[train_and_evaluate_model] Starting model.fit for {model_name}...\")\n    history = model.fit(\n        X_train_arr, y_train,\n        validation_data=(X_val_arr, y_val),\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        callbacks=[checkpoint, early, reduce_lr, thresh_cb],\n        class_weight=cw,\n        verbose=2\n    )\n\n    # Load best weights\n    try:\n        model.load_weights(model_path)\n    except Exception:\n        pass\n\n    # Determine final threshold\n    if os.path.exists(thresh_path):\n        with open(thresh_path, \"r\") as f:\n            js = json.load(f)\n            best_threshold = js.get(\"best_threshold\", 0.5)\n    else:\n        best_threshold = 0.5\n\n    print(f\"[train_and_evaluate_model] Final selected threshold for {model_name}: {best_threshold}\")\n\n    # Evaluate on test\n    y_proba_test = model.predict(X_test_arr, batch_size=64).ravel()\n    metrics_threshold = evaluate_at_threshold(y_test, y_proba_test, best_threshold)\n    print(f\"[train_and_evaluate_model] Test metrics for {model_name} at threshold {best_threshold:.3f}:\")\n    for k, v in metrics_threshold.items():\n        print(f\"  {k}: {v:.4f}\")\n\n    # Save results\n    result = {\n        \"name\": f\"{model_name}_{'robust' if aggressive else 'baseline'}\",\n        \"model_path\": model_path,\n        \"threshold\": best_threshold,\n        \"metrics\": metrics_threshold,\n        \"history\": history.history,\n        \"architecture\": model_name\n    }\n\n    # Persist metrics CSV\n    df = pd.DataFrame([{\n        \"model\": model_name,\n        \"strategy\": \"robust\" if aggressive else \"baseline\",\n        \"threshold\": best_threshold,\n        **metrics_threshold\n    }])\n    csv_path = os.path.join(ANALYSIS_DIR, f\"results_{model_name}_{'robust' if aggressive else 'baseline'}.csv\")\n    df.to_csv(csv_path, index=False)\n    print(f\"[train_and_evaluate_model] Saved metrics to {csv_path}\")\n\n    return result, model, X_test_arr, y_test, y_proba_test\n\n# --------------------------\n# Comparison & Visualization\n# --------------------------\n\ndef compare_and_plot_results(results_list: List[dict], out_csv: str = None):\n    rows = []\n    for res in results_list:\n        r = {\n            \"model\": res[\"name\"],\n            \"architecture\": res[\"architecture\"],\n            \"strategy\": \"robust\" if \"robust\" in res[\"name\"] else \"baseline\",\n            \"threshold\": res[\"threshold\"],\n            **res[\"metrics\"]\n        }\n        rows.append(r)\n    df = pd.DataFrame(rows)\n    if out_csv:\n        df.to_csv(out_csv, index=False)\n    print(\"\\n[compare_and_plot_results] Comparison table:\")\n    print(df)\n\n    # Plot comparison\n    metrics_to_plot = [\"accuracy\", \"f1\", \"minority_f1\", \"minority_recall\"]\n    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n    axs = axs.ravel()\n    \n    for i, m in enumerate(metrics_to_plot):\n        # Separate baseline and robust\n        baseline_data = df[df['strategy'] == 'baseline']\n        robust_data = df[df['strategy'] == 'robust']\n        \n        x = np.arange(len(baseline_data))\n        width = 0.35\n        \n        axs[i].bar(x - width/2, baseline_data[m], width, label='Baseline', alpha=0.7)\n        axs[i].bar(x + width/2, robust_data[m], width, label='Robust', alpha=0.7)\n        \n        axs[i].set_title(f'{m.replace(\"_\", \" \").title()}')\n        axs[i].set_ylabel(m.replace(\"_\", \" \").title())\n        axs[i].set_xlabel('Models')\n        axs[i].set_xticks(x)\n        axs[i].set_xticklabels([arch.split('_')[0] for arch in baseline_data['architecture']], rotation=45)\n        axs[i].legend()\n        axs[i].grid(True, alpha=0.3)\n        \n        # Add value labels\n        for j, v in enumerate(baseline_data[m]):\n            axs[i].text(j - width/2, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=8)\n        for j, v in enumerate(robust_data[m]):\n            axs[i].text(j + width/2, v + 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=8)\n\n    plt.tight_layout()\n    plot_path = os.path.join(VIZ_DIR, \"all_models_comparison.png\")\n    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n    print(f\"[compare_and_plot_results] Saved comparison plot to {plot_path}\")\n    \n    return df, plot_path\n\n# --------------------------\n# Main Execution\n# --------------------------\n\ndef main():\n    print(\"=== Robust Imbalance Handling Pipeline: Start ===\")\n    \n    # Define all models to train\n    MODEL_CONFIGS = [\n        (\"CNN_BiLSTM\", create_cnn_bilstm_model),\n        (\"CNN_Only\", create_cnn_only_model), \n        (\"LSTM\", create_lstm_model),\n        (\"Transformer\", create_lightweight_transformer),\n        (\"Hybrid_CNN_LSTM\", create_hybrid_cnn_lstm_model)\n    ]\n    \n    # Step 1: prepare dataset\n    X_feats, X_waves, y, filenames = prepare_dataset_from_directory(INPUT_AUDIO_DIR)\n    d, ratio = print_class_distribution(y, prefix=\"Full Dataset\")\n    \n    all_results = []\n    \n    # Step 2: Train all models with both baseline and robust strategies\n    for model_name, model_creator in MODEL_CONFIGS:\n        print(f\"\\n{'='*60}\")\n        print(f\"TRAINING MODEL: {model_name}\")\n        print(f\"{'='*60}\")\n        \n        # Baseline training\n        result_baseline, model_baseline, _, _, _ = train_and_evaluate_model(\n            X_feats, y, model_name, model_creator, aggressive=False\n        )\n        all_results.append(result_baseline)\n        \n        # Robust training  \n        result_robust, model_robust, _, _, _ = train_and_evaluate_model(\n            X_feats, y, model_name, model_creator, aggressive=True\n        )\n        all_results.append(result_robust)\n    \n    # Step 3: Compare all results\n    compare_csv = os.path.join(ANALYSIS_DIR, \"all_models_comparison.csv\")\n    df_compare, plot_path = compare_and_plot_results(all_results, out_csv=compare_csv)\n    print(f\"[main] Saved comparison CSV to {compare_csv}\")\n    \n    # Final summary\n    print(\"\\n=== FINAL SUMMARY ===\")\n    print(\"Model Performance Ranking by Minority F1 (Robust Strategy):\")\n    \n    robust_results = [r for r in all_results if \"robust\" in r[\"name\"]]\n    robust_results_sorted = sorted(robust_results, key=lambda x: x[\"metrics\"][\"minority_f1\"], reverse=True)\n    \n    for i, result in enumerate(robust_results_sorted, 1):\n        metrics = result[\"metrics\"]\n        print(f\"{i:2d}. {result['architecture']:15} -> \"\n              f\"Minority F1: {metrics['minority_f1']:.4f}, \"\n              f\"Accuracy: {metrics['accuracy']:.4f}, \"\n              f\"Overall F1: {metrics['f1']:.4f}\")\n    \n    # Calculate average improvements\n    baseline_minor_f1 = np.mean([r[\"metrics\"][\"minority_f1\"] for r in all_results if \"baseline\" in r[\"name\"]])\n    robust_minor_f1 = np.mean([r[\"metrics\"][\"minority_f1\"] for r in all_results if \"robust\" in r[\"name\"]])\n    improvement = robust_minor_f1 - baseline_minor_f1\n    \n    print(f\"\\nAverage Improvement with Robust Strategy:\")\n    print(f\"  Baseline average minority F1: {baseline_minor_f1:.4f}\")\n    print(f\"  Robust average minority F1: {robust_minor_f1:.4f}\")\n    print(f\"  Improvement: {improvement:.4f} ({improvement*100:.2f}%)\")\n    \n    if robust_minor_f1 > baseline_minor_f1:\n        print(\"=> Robust strategy successfully improved minority class detection ✅\")\n    else:\n        print(\"=> No overall improvement with robust strategy ❗\")\n    \n    print(\"\\nSaved artifacts:\")\n    print(f\"  - Models: {MODEL_DIR}\")\n    print(f\"  - Analysis CSV: {compare_csv}\")\n    print(f\"  - Visualizations: {plot_path}\")\n    print(f\"  - Total models trained: {len(all_results)}\")\n\n    print(\"\\n=== Pipeline Complete ===\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T16:54:47.091976Z","iopub.execute_input":"2025-11-01T16:54:47.092326Z","iopub.status.idle":"2025-11-01T17:30:42.367122Z","shell.execute_reply.started":"2025-11-01T16:54:47.092301Z","shell.execute_reply":"2025-11-01T17:30:42.366353Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"[CUDA] Found 1 GPU(s). Enabled memory growth.\n=== Robust Imbalance Handling Pipeline: Start ===\n[prepare_dataset_from_directory] Scanning audio directory and extracting features...\n[prepare_dataset_from_directory] Loaded 7442 audio files.\n\n[Full Dataset] Class distribution:\n  Class 0: 2358 samples\n  Class 1: 5084 samples\n  Imbalance ratio (maj/min): 2.156\n\n============================================================\nTRAINING MODEL: CNN_BiLSTM\n============================================================\n\n[train_and_evaluate_model] Starting training for 'CNN_BiLSTM' (aggressive=False)\n\n[Train-CNN_BiLSTM] Class distribution:\n  Class 0: 1650 samples\n  Class 1: 3558 samples\n  Imbalance ratio (maj/min): 2.156\n\n[Val-CNN_BiLSTM] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n\n[Test-CNN_BiLSTM] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n[train_and_evaluate_model] Target time frames for model input: 94\n[train_and_evaluate_model] Class weights used for training: {0: 1.5781818181818181, 1: 0.7318718381112985}\n[train_and_evaluate_model] Using BinaryCrossentropy loss (baseline).\n\n[Model Architecture - CNN_BiLSTM]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_mfcc (\u001b[38;5;33mInputLayer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ cast_2 (\u001b[38;5;33mCast\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_0 (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m7,744\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_1 (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m24,704\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_2 (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m98,560\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m394,240\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m164,352\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ out (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_mfcc (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ cast_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,744</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m716,225\u001b[0m (2.73 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">716,225</span> (2.73 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m715,329\u001b[0m (2.73 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">715,329</span> (2.73 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"[train_and_evaluate_model] Starting model.fit for CNN_BiLSTM...\nEpoch 1/30\n\nEpoch 1: val_loss improved from inf to 0.87710, saving model to ./BASE_DIR/models/robust_models/model_CNN_BiLSTM_baseline.h5\n163/163 - 16s - 99ms/step - auc: 0.7051 - binary_accuracy: 0.6386 - loss: 0.6273 - precision: 0.8104 - recall: 0.6150 - val_auc: 0.5417 - val_binary_accuracy: 0.6312 - val_loss: 0.8771 - val_precision: 0.6692 - val_recall: 0.9096 - learning_rate: 0.0010\nEpoch 2/30\n\nEpoch 2: val_loss improved from 0.87710 to 0.66118, saving model to ./BASE_DIR/models/robust_models/model_CNN_BiLSTM_baseline.h5\n[ThresholdOptimizer] Searching threshold at epoch 2 ...\n[ThresholdOptimizer] Best threshold so far: 0.210 with minority F1=0.8143\n163/163 - 5s - 32ms/step - auc: 0.7493 - binary_accuracy: 0.6715 - loss: 0.5921 - precision: 0.8468 - recall: 0.6338 - val_auc: 0.6579 - val_binary_accuracy: 0.6410 - val_loss: 0.6612 - val_precision: 0.7291 - val_recall: 0.7549 - learning_rate: 0.0010\nEpoch 3/30\n\nEpoch 3: val_loss improved from 0.66118 to 0.56231, saving model to ./BASE_DIR/models/robust_models/model_CNN_BiLSTM_baseline.h5\n163/163 - 3s - 20ms/step - auc: 0.7765 - binary_accuracy: 0.6964 - loss: 0.5660 - precision: 0.8578 - recall: 0.6661 - val_auc: 0.7712 - val_binary_accuracy: 0.6795 - val_loss: 0.5623 - val_precision: 0.8534 - val_recall: 0.6409 - learning_rate: 0.0010\nEpoch 4/30\n\nEpoch 4: val_loss improved from 0.56231 to 0.55203, saving model to ./BASE_DIR/models/robust_models/model_CNN_BiLSTM_baseline.h5\n[ThresholdOptimizer] Searching threshold at epoch 4 ...\n[ThresholdOptimizer] Best threshold so far: 0.240 with minority F1=0.8289\n163/163 - 3s - 21ms/step - auc: 0.8047 - binary_accuracy: 0.7127 - loss: 0.5354 - precision: 0.8819 - recall: 0.6692 - val_auc: 0.7784 - val_binary_accuracy: 0.7225 - val_loss: 0.5520 - val_precision: 0.8278 - val_recall: 0.7497 - learning_rate: 0.0010\nEpoch 5/30\n\nEpoch 5: val_loss improved from 0.55203 to 0.53981, saving model to ./BASE_DIR/models/robust_models/model_CNN_BiLSTM_baseline.h5\n163/163 - 3s - 19ms/step - auc: 0.8203 - binary_accuracy: 0.7339 - loss: 0.5175 - precision: 0.8784 - recall: 0.7085 - val_auc: 0.8291 - val_binary_accuracy: 0.7296 - val_loss: 0.5398 - val_precision: 0.8773 - val_recall: 0.7025 - learning_rate: 0.0010\nEpoch 6/30\n\nEpoch 6: val_loss improved from 0.53981 to 0.48593, saving model to ./BASE_DIR/models/robust_models/model_CNN_BiLSTM_baseline.h5\n[ThresholdOptimizer] Searching threshold at epoch 6 ...\n[ThresholdOptimizer] Best threshold so far: 0.250 with minority F1=0.8453\n163/163 - 3s - 21ms/step - auc: 0.8393 - binary_accuracy: 0.7485 - loss: 0.4885 - precision: 0.8905 - recall: 0.7203 - val_auc: 0.8380 - val_binary_accuracy: 0.7431 - val_loss: 0.4859 - val_precision: 0.8754 - val_recall: 0.7274 - learning_rate: 0.0010\nEpoch 7/30\n\nEpoch 7: val_loss did not improve from 0.48593\n163/163 - 3s - 19ms/step - auc: 0.8452 - binary_accuracy: 0.7454 - loss: 0.4809 - precision: 0.9052 - recall: 0.7007 - val_auc: 0.8359 - val_binary_accuracy: 0.7404 - val_loss: 0.4909 - val_precision: 0.8713 - val_recall: 0.7274 - learning_rate: 0.0010\nEpoch 8/30\n\nEpoch 8: val_loss did not improve from 0.48593\n[ThresholdOptimizer] Searching threshold at epoch 8 ...\n[ThresholdOptimizer] Best threshold so far: 0.160 with minority F1=0.8566\n163/163 - 3s - 21ms/step - auc: 0.8567 - binary_accuracy: 0.7609 - loss: 0.4658 - precision: 0.9144 - recall: 0.7173 - val_auc: 0.8384 - val_binary_accuracy: 0.6679 - val_loss: 0.5945 - val_precision: 0.9242 - val_recall: 0.5596 - learning_rate: 0.0010\nEpoch 9/30\n\nEpoch 9: val_loss improved from 0.48593 to 0.48543, saving model to ./BASE_DIR/models/robust_models/model_CNN_BiLSTM_baseline.h5\n163/163 - 3s - 20ms/step - auc: 0.8653 - binary_accuracy: 0.7727 - loss: 0.4494 - precision: 0.9227 - recall: 0.7282 - val_auc: 0.8245 - val_binary_accuracy: 0.7645 - val_loss: 0.4854 - val_precision: 0.8463 - val_recall: 0.8008 - learning_rate: 0.0010\nEpoch 10/30\n\nEpoch 10: val_loss did not improve from 0.48543\n[ThresholdOptimizer] Searching threshold at epoch 10 ...\n[ThresholdOptimizer] Best threshold so far: 0.160 with minority F1=0.8566\n163/163 - 4s - 22ms/step - auc: 0.8687 - binary_accuracy: 0.7696 - loss: 0.4465 - precision: 0.9140 - recall: 0.7316 - val_auc: 0.8261 - val_binary_accuracy: 0.6723 - val_loss: 0.6156 - val_precision: 0.9026 - val_recall: 0.5832 - learning_rate: 0.0010\nEpoch 11/30\n\nEpoch 11: val_loss did not improve from 0.48543\n163/163 - 3s - 21ms/step - auc: 0.8839 - binary_accuracy: 0.7897 - loss: 0.4204 - precision: 0.9222 - recall: 0.7560 - val_auc: 0.8570 - val_binary_accuracy: 0.7028 - val_loss: 0.5523 - val_precision: 0.9184 - val_recall: 0.6199 - learning_rate: 0.0010\nEpoch 12/30\n\nEpoch 12: val_loss improved from 0.48543 to 0.47781, saving model to ./BASE_DIR/models/robust_models/model_CNN_BiLSTM_baseline.h5\n[ThresholdOptimizer] Searching threshold at epoch 12 ...\n[ThresholdOptimizer] Best threshold so far: 0.160 with minority F1=0.8566\n163/163 - 4s - 23ms/step - auc: 0.8928 - binary_accuracy: 0.7990 - loss: 0.4035 - precision: 0.9304 - recall: 0.7628 - val_auc: 0.8442 - val_binary_accuracy: 0.7762 - val_loss: 0.4778 - val_precision: 0.8388 - val_recall: 0.8322 - learning_rate: 0.0010\nEpoch 13/30\n\nEpoch 13: val_loss did not improve from 0.47781\n163/163 - 3s - 21ms/step - auc: 0.8931 - binary_accuracy: 0.7993 - loss: 0.4022 - precision: 0.9358 - recall: 0.7583 - val_auc: 0.8412 - val_binary_accuracy: 0.7457 - val_loss: 0.5547 - val_precision: 0.8844 - val_recall: 0.7221 - learning_rate: 0.0010\nEpoch 14/30\n\nEpoch 14: val_loss did not improve from 0.47781\n[ThresholdOptimizer] Searching threshold at epoch 14 ...\n[ThresholdOptimizer] Best threshold so far: 0.110 with minority F1=0.8611\n163/163 - 4s - 22ms/step - auc: 0.9034 - binary_accuracy: 0.8072 - loss: 0.3824 - precision: 0.9352 - recall: 0.7712 - val_auc: 0.8576 - val_binary_accuracy: 0.7153 - val_loss: 0.5864 - val_precision: 0.9174 - val_recall: 0.6409 - learning_rate: 0.0010\nEpoch 15/30\n\nEpoch 15: val_loss did not improve from 0.47781\n\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n163/163 - 3s - 21ms/step - auc: 0.9117 - binary_accuracy: 0.8241 - loss: 0.3647 - precision: 0.9418 - recall: 0.7915 - val_auc: 0.8355 - val_binary_accuracy: 0.7090 - val_loss: 0.6148 - val_precision: 0.9179 - val_recall: 0.6304 - learning_rate: 0.0010\nEpoch 16/30\n\nEpoch 16: val_loss did not improve from 0.47781\n[ThresholdOptimizer] Searching threshold at epoch 16 ...\n[ThresholdOptimizer] Best threshold so far: 0.240 with minority F1=0.8688\n163/163 - 4s - 22ms/step - auc: 0.9274 - binary_accuracy: 0.8425 - loss: 0.3320 - precision: 0.9530 - recall: 0.8094 - val_auc: 0.8679 - val_binary_accuracy: 0.7959 - val_loss: 0.5025 - val_precision: 0.8838 - val_recall: 0.8073 - learning_rate: 5.0000e-04\nEpoch 17/30\n\nEpoch 17: val_loss did not improve from 0.47781\n163/163 - 3s - 21ms/step - auc: 0.9389 - binary_accuracy: 0.8593 - loss: 0.3012 - precision: 0.9558 - recall: 0.8325 - val_auc: 0.8679 - val_binary_accuracy: 0.7923 - val_loss: 0.5689 - val_precision: 0.8899 - val_recall: 0.7942 - learning_rate: 5.0000e-04\nEpoch 18/30\n\nEpoch 18: val_loss did not improve from 0.47781\n\nEpoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n[ThresholdOptimizer] Searching threshold at epoch 18 ...\n[ThresholdOptimizer] Best threshold so far: 0.250 with minority F1=0.8694\n163/163 - 4s - 22ms/step - auc: 0.9372 - binary_accuracy: 0.8514 - loss: 0.3032 - precision: 0.9558 - recall: 0.8204 - val_auc: 0.8675 - val_binary_accuracy: 0.7986 - val_loss: 0.5279 - val_precision: 0.8799 - val_recall: 0.8165 - learning_rate: 5.0000e-04\nEpoch 18: early stopping\nRestoring model weights from the end of the best epoch: 12.\n[train_and_evaluate_model] Final selected threshold for CNN_BiLSTM: 0.25\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n[train_and_evaluate_model] Test metrics for CNN_BiLSTM at threshold 0.250:\n  accuracy: 0.7449\n  precision: 0.7638\n  recall: 0.9069\n  f1: 0.8292\n  minority_precision: 0.7638\n  minority_recall: 0.9069\n  minority_f1: 0.8292\n[train_and_evaluate_model] Saved metrics to ./BASE_DIR/imbalance_analysis/results_CNN_BiLSTM_baseline.csv\n\n[train_and_evaluate_model] Starting training for 'CNN_BiLSTM' (aggressive=True)\n\n[Train-CNN_BiLSTM] Class distribution:\n  Class 0: 1650 samples\n  Class 1: 3558 samples\n  Imbalance ratio (maj/min): 2.156\n\n[Val-CNN_BiLSTM] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n\n[Test-CNN_BiLSTM] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n[train_and_evaluate_model] Target time frames for model input: 94\n[train_and_evaluate_model] Applying heavy spec_augment to minority samples for CNN_BiLSTM...\n[train_and_evaluate_model] Created 10674 augmented MFCC samples for minority.\n[train_and_evaluate_model] New training size: 15882\n[train_and_evaluate_model] Class weights used for training: {0: 4.8127272727272725, 1: 0.5579679595278246}\n[train_and_evaluate_model] Using dynamic focal loss with alpha_pos=0.5000\n\n[Model Architecture - CNN_BiLSTM]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_mfcc (\u001b[38;5;33mInputLayer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ cast_3 (\u001b[38;5;33mCast\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_0 (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m7,744\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_6 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_1 (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m24,704\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_7 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_2 (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m98,560\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_8 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m394,240\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_5 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m164,352\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ out (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_mfcc (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ cast_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,744</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m716,225\u001b[0m (2.73 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">716,225</span> (2.73 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m715,329\u001b[0m (2.73 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">715,329</span> (2.73 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"[train_and_evaluate_model] Starting model.fit for CNN_BiLSTM...\nEpoch 1/30\n\nEpoch 1: val_loss improved from inf to 0.07120, saving model to ./BASE_DIR/models/robust_models/model_CNN_BiLSTM_robust.h5\n497/497 - 22s - 44ms/step - auc: 0.8946 - binary_accuracy: 0.8893 - loss: 0.0303 - precision: 0.9126 - recall: 0.9693 - val_auc: 0.7388 - val_binary_accuracy: 0.7010 - val_loss: 0.0712 - val_precision: 0.7191 - val_recall: 0.9227 - learning_rate: 0.0010\nEpoch 2/30\n\nEpoch 2: val_loss did not improve from 0.07120\n[ThresholdOptimizer] Searching threshold at epoch 2 ...\n[ThresholdOptimizer] Best threshold so far: 0.520 with minority F1=0.8152\n497/497 - 12s - 24ms/step - auc: 0.9158 - binary_accuracy: 0.8915 - loss: 0.0266 - precision: 0.9105 - recall: 0.9748 - val_auc: 0.7513 - val_binary_accuracy: 0.6831 - val_loss: 0.0720 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 3/30\n\nEpoch 3: val_loss improved from 0.07120 to 0.06863, saving model to ./BASE_DIR/models/robust_models/model_CNN_BiLSTM_robust.h5\n497/497 - 10s - 21ms/step - auc: 0.9273 - binary_accuracy: 0.8972 - loss: 0.0253 - precision: 0.9208 - recall: 0.9686 - val_auc: 0.7620 - val_binary_accuracy: 0.7162 - val_loss: 0.0686 - val_precision: 0.7253 - val_recall: 0.9410 - learning_rate: 0.0010\nEpoch 4/30\n\nEpoch 4: val_loss improved from 0.06863 to 0.06709, saving model to ./BASE_DIR/models/robust_models/model_CNN_BiLSTM_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 4 ...\n[ThresholdOptimizer] Best threshold so far: 0.510 with minority F1=0.8329\n497/497 - 10s - 21ms/step - auc: 0.9281 - binary_accuracy: 0.8962 - loss: 0.0251 - precision: 0.9256 - recall: 0.9614 - val_auc: 0.7783 - val_binary_accuracy: 0.7162 - val_loss: 0.0671 - val_precision: 0.7173 - val_recall: 0.9646 - learning_rate: 0.0010\nEpoch 5/30\n\nEpoch 5: val_loss improved from 0.06709 to 0.06675, saving model to ./BASE_DIR/models/robust_models/model_CNN_BiLSTM_robust.h5\n497/497 - 10s - 20ms/step - auc: 0.9359 - binary_accuracy: 0.9056 - loss: 0.0241 - precision: 0.9377 - recall: 0.9584 - val_auc: 0.7834 - val_binary_accuracy: 0.7466 - val_loss: 0.0668 - val_precision: 0.7878 - val_recall: 0.8611 - learning_rate: 0.0010\nEpoch 6/30\n\nEpoch 6: val_loss improved from 0.06675 to 0.06623, saving model to ./BASE_DIR/models/robust_models/model_CNN_BiLSTM_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 6 ...\n[ThresholdOptimizer] Best threshold so far: 0.510 with minority F1=0.8329\n497/497 - 10s - 21ms/step - auc: 0.9371 - binary_accuracy: 0.9064 - loss: 0.0243 - precision: 0.9385 - recall: 0.9583 - val_auc: 0.7821 - val_binary_accuracy: 0.7386 - val_loss: 0.0662 - val_precision: 0.7848 - val_recall: 0.8506 - learning_rate: 0.0010\nEpoch 7/30\n\nEpoch 7: val_loss improved from 0.06623 to 0.06606, saving model to ./BASE_DIR/models/robust_models/model_CNN_BiLSTM_robust.h5\n497/497 - 10s - 20ms/step - auc: 0.9402 - binary_accuracy: 0.9101 - loss: 0.0236 - precision: 0.9407 - recall: 0.9603 - val_auc: 0.7939 - val_binary_accuracy: 0.7592 - val_loss: 0.0661 - val_precision: 0.7940 - val_recall: 0.8742 - learning_rate: 0.0010\nEpoch 8/30\n\nEpoch 8: val_loss improved from 0.06606 to 0.06267, saving model to ./BASE_DIR/models/robust_models/model_CNN_BiLSTM_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 8 ...\n[ThresholdOptimizer] Best threshold so far: 0.500 with minority F1=0.8441\n497/497 - 10s - 21ms/step - auc: 0.9434 - binary_accuracy: 0.9123 - loss: 0.0233 - precision: 0.9444 - recall: 0.9585 - val_auc: 0.8124 - val_binary_accuracy: 0.7672 - val_loss: 0.0627 - val_precision: 0.7779 - val_recall: 0.9227 - learning_rate: 0.0010\nEpoch 9/30\n\nEpoch 9: val_loss did not improve from 0.06267\n497/497 - 10s - 20ms/step - auc: 0.9499 - binary_accuracy: 0.9166 - loss: 0.0220 - precision: 0.9482 - recall: 0.9593 - val_auc: 0.7934 - val_binary_accuracy: 0.7565 - val_loss: 0.0669 - val_precision: 0.7933 - val_recall: 0.8702 - learning_rate: 0.0010\nEpoch 10/30\n\nEpoch 10: val_loss did not improve from 0.06267\n[ThresholdOptimizer] Searching threshold at epoch 10 ...\n[ThresholdOptimizer] Best threshold so far: 0.500 with minority F1=0.8548\n497/497 - 10s - 21ms/step - auc: 0.9509 - binary_accuracy: 0.9198 - loss: 0.0221 - precision: 0.9497 - recall: 0.9614 - val_auc: 0.8242 - val_binary_accuracy: 0.7807 - val_loss: 0.0656 - val_precision: 0.7785 - val_recall: 0.9489 - learning_rate: 0.0010\nEpoch 11/30\n\nEpoch 11: val_loss did not improve from 0.06267\n\nEpoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n497/497 - 10s - 20ms/step - auc: 0.9512 - binary_accuracy: 0.9181 - loss: 0.0219 - precision: 0.9546 - recall: 0.9539 - val_auc: 0.8230 - val_binary_accuracy: 0.7368 - val_loss: 0.0691 - val_precision: 0.7333 - val_recall: 0.9659 - learning_rate: 0.0010\nEpoch 12/30\n\nEpoch 12: val_loss improved from 0.06267 to 0.05998, saving model to ./BASE_DIR/models/robust_models/model_CNN_BiLSTM_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 12 ...\n[ThresholdOptimizer] Best threshold so far: 0.500 with minority F1=0.8548\n497/497 - 10s - 21ms/step - auc: 0.9597 - binary_accuracy: 0.9273 - loss: 0.0202 - precision: 0.9622 - recall: 0.9565 - val_auc: 0.8384 - val_binary_accuracy: 0.7878 - val_loss: 0.0600 - val_precision: 0.8247 - val_recall: 0.8755 - learning_rate: 5.0000e-04\nEpoch 13/30\n\nEpoch 13: val_loss did not improve from 0.05998\n497/497 - 10s - 20ms/step - auc: 0.9619 - binary_accuracy: 0.9275 - loss: 0.0196 - precision: 0.9633 - recall: 0.9555 - val_auc: 0.8285 - val_binary_accuracy: 0.7681 - val_loss: 0.0651 - val_precision: 0.7986 - val_recall: 0.8834 - learning_rate: 5.0000e-04\nEpoch 14/30\n\nEpoch 14: val_loss did not improve from 0.05998\n[ThresholdOptimizer] Searching threshold at epoch 14 ...\n[ThresholdOptimizer] Best threshold so far: 0.500 with minority F1=0.8548\n497/497 - 10s - 21ms/step - auc: 0.9651 - binary_accuracy: 0.9317 - loss: 0.0189 - precision: 0.9678 - recall: 0.9557 - val_auc: 0.8502 - val_binary_accuracy: 0.7869 - val_loss: 0.0654 - val_precision: 0.8056 - val_recall: 0.9069 - learning_rate: 5.0000e-04\nEpoch 15/30\n\nEpoch 15: val_loss did not improve from 0.05998\n\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n497/497 - 10s - 20ms/step - auc: 0.9679 - binary_accuracy: 0.9354 - loss: 0.0183 - precision: 0.9686 - recall: 0.9590 - val_auc: 0.8467 - val_binary_accuracy: 0.7833 - val_loss: 0.0883 - val_precision: 0.7950 - val_recall: 0.9201 - learning_rate: 5.0000e-04\nEpoch 16/30\n\nEpoch 16: val_loss did not improve from 0.05998\n[ThresholdOptimizer] Searching threshold at epoch 16 ...\n[ThresholdOptimizer] Best threshold so far: 0.490 with minority F1=0.8582\n497/497 - 10s - 21ms/step - auc: 0.9717 - binary_accuracy: 0.9392 - loss: 0.0171 - precision: 0.9727 - recall: 0.9591 - val_auc: 0.8489 - val_binary_accuracy: 0.7932 - val_loss: 0.0718 - val_precision: 0.8122 - val_recall: 0.9069 - learning_rate: 2.5000e-04\nEpoch 17/30\n\nEpoch 17: val_loss did not improve from 0.05998\n497/497 - 10s - 20ms/step - auc: 0.9725 - binary_accuracy: 0.9399 - loss: 0.0170 - precision: 0.9708 - recall: 0.9618 - val_auc: 0.8551 - val_binary_accuracy: 0.7842 - val_loss: 0.0721 - val_precision: 0.8092 - val_recall: 0.8952 - learning_rate: 2.5000e-04\nEpoch 18/30\n\nEpoch 18: val_loss did not improve from 0.05998\n\nEpoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n[ThresholdOptimizer] Searching threshold at epoch 18 ...\n[ThresholdOptimizer] Best threshold so far: 0.500 with minority F1=0.8663\n497/497 - 10s - 21ms/step - auc: 0.9737 - binary_accuracy: 0.9420 - loss: 0.0165 - precision: 0.9735 - recall: 0.9614 - val_auc: 0.8603 - val_binary_accuracy: 0.8102 - val_loss: 0.0617 - val_precision: 0.8348 - val_recall: 0.9004 - learning_rate: 2.5000e-04\nEpoch 18: early stopping\nRestoring model weights from the end of the best epoch: 12.\n[train_and_evaluate_model] Final selected threshold for CNN_BiLSTM: 0.5\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n[train_and_evaluate_model] Test metrics for CNN_BiLSTM at threshold 0.500:\n  accuracy: 0.7449\n  precision: 0.8048\n  recall: 0.8270\n  f1: 0.8158\n  minority_precision: 0.8048\n  minority_recall: 0.8270\n  minority_f1: 0.8158\n[train_and_evaluate_model] Saved metrics to ./BASE_DIR/imbalance_analysis/results_CNN_BiLSTM_robust.csv\n\n============================================================\nTRAINING MODEL: CNN_Only\n============================================================\n\n[train_and_evaluate_model] Starting training for 'CNN_Only' (aggressive=False)\n\n[Train-CNN_Only] Class distribution:\n  Class 0: 1650 samples\n  Class 1: 3558 samples\n  Imbalance ratio (maj/min): 2.156\n\n[Val-CNN_Only] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n\n[Test-CNN_Only] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n[train_and_evaluate_model] Target time frames for model input: 94\n[train_and_evaluate_model] Class weights used for training: {0: 1.5781818181818181, 1: 0.7318718381112985}\n[train_and_evaluate_model] Using BinaryCrossentropy loss (baseline).\n\n[Model Architecture - CNN_Only]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_3\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_mfcc (\u001b[38;5;33mInputLayer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ cast_4 (\u001b[38;5;33mCast\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m12,864\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_9 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m41,088\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_10 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m98,560\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_11 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_17 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m393,728\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_13          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_12 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ out (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_mfcc (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ cast_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,864</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">393,728</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_13          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m723,649\u001b[0m (2.76 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">723,649</span> (2.76 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m721,217\u001b[0m (2.75 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">721,217</span> (2.75 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,432\u001b[0m (9.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> (9.50 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"[train_and_evaluate_model] Starting model.fit for CNN_Only...\nEpoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1762016443.644419      98 service.cc:148] XLA service 0x789e224ff610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1762016443.645123      98 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1762016451.596586      98 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: val_loss improved from inf to 0.69014, saving model to ./BASE_DIR/models/robust_models/model_CNN_Only_baseline.h5\n163/163 - 31s - 192ms/step - auc: 0.6509 - binary_accuracy: 0.5968 - loss: 0.7131 - precision: 0.7782 - recall: 0.5731 - val_auc: 0.5653 - val_binary_accuracy: 0.6777 - val_loss: 0.6901 - val_precision: 0.6814 - val_recall: 0.9921 - learning_rate: 0.0010\nEpoch 2/30\n\nEpoch 2: val_loss did not improve from 0.69014\n[ThresholdOptimizer] Searching threshold at epoch 2 ...\n[ThresholdOptimizer] Best threshold so far: 0.000 with minority F1=0.8117\n163/163 - 3s - 18ms/step - auc: 0.7084 - binary_accuracy: 0.6336 - loss: 0.6294 - precision: 0.8156 - recall: 0.5992 - val_auc: 0.5920 - val_binary_accuracy: 0.5765 - val_loss: 0.7168 - val_precision: 0.6597 - val_recall: 0.7851 - learning_rate: 0.0010\nEpoch 3/30\n\nEpoch 3: val_loss improved from 0.69014 to 0.67146, saving model to ./BASE_DIR/models/robust_models/model_CNN_Only_baseline.h5\n163/163 - 1s - 7ms/step - auc: 0.7416 - binary_accuracy: 0.6559 - loss: 0.6009 - precision: 0.8388 - recall: 0.6144 - val_auc: 0.6491 - val_binary_accuracy: 0.5971 - val_loss: 0.6715 - val_precision: 0.7630 - val_recall: 0.5950 - learning_rate: 0.0010\nEpoch 4/30\n\nEpoch 4: val_loss improved from 0.67146 to 0.61515, saving model to ./BASE_DIR/models/robust_models/model_CNN_Only_baseline.h5\n[ThresholdOptimizer] Searching threshold at epoch 4 ...\n[ThresholdOptimizer] Best threshold so far: 0.220 with minority F1=0.8121\n163/163 - 1s - 9ms/step - auc: 0.7626 - binary_accuracy: 0.6803 - loss: 0.5817 - precision: 0.8509 - recall: 0.6450 - val_auc: 0.6737 - val_binary_accuracy: 0.6356 - val_loss: 0.6151 - val_precision: 0.7380 - val_recall: 0.7235 - learning_rate: 0.0010\nEpoch 5/30\n\nEpoch 5: val_loss improved from 0.61515 to 0.51428, saving model to ./BASE_DIR/models/robust_models/model_CNN_Only_baseline.h5\n163/163 - 1s - 7ms/step - auc: 0.7758 - binary_accuracy: 0.6995 - loss: 0.5692 - precision: 0.8594 - recall: 0.6698 - val_auc: 0.7838 - val_binary_accuracy: 0.7261 - val_loss: 0.5143 - val_precision: 0.7889 - val_recall: 0.8178 - learning_rate: 0.0010\nEpoch 6/30\n\nEpoch 6: val_loss did not improve from 0.51428\n[ThresholdOptimizer] Searching threshold at epoch 6 ...\n[ThresholdOptimizer] Best threshold so far: 0.190 with minority F1=0.8122\n163/163 - 1s - 8ms/step - auc: 0.7919 - binary_accuracy: 0.7081 - loss: 0.5535 - precision: 0.8676 - recall: 0.6759 - val_auc: 0.7433 - val_binary_accuracy: 0.6974 - val_loss: 0.5636 - val_precision: 0.8139 - val_recall: 0.7221 - learning_rate: 0.0010\nEpoch 7/30\n\nEpoch 7: val_loss did not improve from 0.51428\n163/163 - 1s - 6ms/step - auc: 0.8150 - binary_accuracy: 0.7231 - loss: 0.5203 - precision: 0.8787 - recall: 0.6900 - val_auc: 0.7867 - val_binary_accuracy: 0.7126 - val_loss: 0.5398 - val_precision: 0.8369 - val_recall: 0.7195 - learning_rate: 0.0010\nEpoch 8/30\n\nEpoch 8: val_loss did not improve from 0.51428\n\nEpoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n[ThresholdOptimizer] Searching threshold at epoch 8 ...\n[ThresholdOptimizer] Best threshold so far: 0.280 with minority F1=0.8262\n163/163 - 1s - 8ms/step - auc: 0.8192 - binary_accuracy: 0.7254 - loss: 0.5240 - precision: 0.8797 - recall: 0.6928 - val_auc: 0.7832 - val_binary_accuracy: 0.6813 - val_loss: 0.5545 - val_precision: 0.8680 - val_recall: 0.6291 - learning_rate: 0.0010\nEpoch 9/30\n\nEpoch 9: val_loss improved from 0.51428 to 0.47668, saving model to ./BASE_DIR/models/robust_models/model_CNN_Only_baseline.h5\n163/163 - 1s - 7ms/step - auc: 0.8420 - binary_accuracy: 0.7389 - loss: 0.4881 - precision: 0.9002 - recall: 0.6948 - val_auc: 0.8159 - val_binary_accuracy: 0.7520 - val_loss: 0.4767 - val_precision: 0.8000 - val_recall: 0.8493 - learning_rate: 5.0000e-04\nEpoch 10/30\n\nEpoch 10: val_loss did not improve from 0.47668\n[ThresholdOptimizer] Searching threshold at epoch 10 ...\n[ThresholdOptimizer] Best threshold so far: 0.300 with minority F1=0.8376\n163/163 - 1s - 8ms/step - auc: 0.8536 - binary_accuracy: 0.7588 - loss: 0.4696 - precision: 0.9033 - recall: 0.7246 - val_auc: 0.8025 - val_binary_accuracy: 0.7574 - val_loss: 0.5319 - val_precision: 0.8083 - val_recall: 0.8453 - learning_rate: 5.0000e-04\nEpoch 11/30\n\nEpoch 11: val_loss did not improve from 0.47668\n163/163 - 1s - 6ms/step - auc: 0.8501 - binary_accuracy: 0.7638 - loss: 0.4747 - precision: 0.9050 - recall: 0.7310 - val_auc: 0.8107 - val_binary_accuracy: 0.7538 - val_loss: 0.5274 - val_precision: 0.8486 - val_recall: 0.7785 - learning_rate: 5.0000e-04\nEpoch 12/30\n\nEpoch 12: val_loss did not improve from 0.47668\n\nEpoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n[ThresholdOptimizer] Searching threshold at epoch 12 ...\n[ThresholdOptimizer] Best threshold so far: 0.300 with minority F1=0.8376\n163/163 - 1s - 8ms/step - auc: 0.8630 - binary_accuracy: 0.7775 - loss: 0.4519 - precision: 0.9140 - recall: 0.7442 - val_auc: 0.7844 - val_binary_accuracy: 0.7135 - val_loss: 0.5906 - val_precision: 0.8445 - val_recall: 0.7117 - learning_rate: 5.0000e-04\nEpoch 13/30\n\nEpoch 13: val_loss did not improve from 0.47668\n163/163 - 1s - 6ms/step - auc: 0.8757 - binary_accuracy: 0.7819 - loss: 0.4331 - precision: 0.9217 - recall: 0.7440 - val_auc: 0.8341 - val_binary_accuracy: 0.7654 - val_loss: 0.5018 - val_precision: 0.8474 - val_recall: 0.8008 - learning_rate: 2.5000e-04\nEpoch 14/30\n\nEpoch 14: val_loss did not improve from 0.47668\n[ThresholdOptimizer] Searching threshold at epoch 14 ...\n[ThresholdOptimizer] Best threshold so far: 0.270 with minority F1=0.8457\n163/163 - 1s - 8ms/step - auc: 0.8855 - binary_accuracy: 0.7909 - loss: 0.4169 - precision: 0.9309 - recall: 0.7496 - val_auc: 0.8186 - val_binary_accuracy: 0.7663 - val_loss: 0.5593 - val_precision: 0.8356 - val_recall: 0.8191 - learning_rate: 2.5000e-04\nEpoch 15/30\n\nEpoch 15: val_loss did not improve from 0.47668\n\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n163/163 - 1s - 6ms/step - auc: 0.8919 - binary_accuracy: 0.7990 - loss: 0.4021 - precision: 0.9313 - recall: 0.7619 - val_auc: 0.8387 - val_binary_accuracy: 0.7807 - val_loss: 0.5504 - val_precision: 0.8190 - val_recall: 0.8716 - learning_rate: 2.5000e-04\nEpoch 15: early stopping\nRestoring model weights from the end of the best epoch: 9.\n[train_and_evaluate_model] Final selected threshold for CNN_Only: 0.27\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n[train_and_evaluate_model] Test metrics for CNN_Only at threshold 0.270:\n  accuracy: 0.7261\n  precision: 0.7247\n  recall: 0.9659\n  f1: 0.8281\n  minority_precision: 0.7247\n  minority_recall: 0.9659\n  minority_f1: 0.8281\n[train_and_evaluate_model] Saved metrics to ./BASE_DIR/imbalance_analysis/results_CNN_Only_baseline.csv\n\n[train_and_evaluate_model] Starting training for 'CNN_Only' (aggressive=True)\n\n[Train-CNN_Only] Class distribution:\n  Class 0: 1650 samples\n  Class 1: 3558 samples\n  Imbalance ratio (maj/min): 2.156\n\n[Val-CNN_Only] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n\n[Test-CNN_Only] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n[train_and_evaluate_model] Target time frames for model input: 94\n[train_and_evaluate_model] Applying heavy spec_augment to minority samples for CNN_Only...\n[train_and_evaluate_model] Created 10674 augmented MFCC samples for minority.\n[train_and_evaluate_model] New training size: 15882\n[train_and_evaluate_model] Class weights used for training: {0: 4.8127272727272725, 1: 0.5579679595278246}\n[train_and_evaluate_model] Using dynamic focal loss with alpha_pos=0.5000\n\n[Model Architecture - CNN_Only]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_4\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_mfcc (\u001b[38;5;33mInputLayer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ cast_5 (\u001b[38;5;33mCast\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m12,864\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_15          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_13 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m41,088\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_16          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_14 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_23 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m98,560\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_17          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_15 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m393,728\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_18          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_16 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m512\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_19          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_28 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ out (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_mfcc (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ cast_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,864</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_15          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_16          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_17          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">393,728</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_18          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_19          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m723,649\u001b[0m (2.76 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">723,649</span> (2.76 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m721,217\u001b[0m (2.75 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">721,217</span> (2.75 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,432\u001b[0m (9.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> (9.50 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"[train_and_evaluate_model] Starting model.fit for CNN_Only...\nEpoch 1/30\n\nEpoch 1: val_loss improved from inf to 0.07853, saving model to ./BASE_DIR/models/robust_models/model_CNN_Only_robust.h5\n497/497 - 28s - 55ms/step - auc: 0.8702 - binary_accuracy: 0.8778 - loss: 0.0361 - precision: 0.9162 - recall: 0.9506 - val_auc: 0.5717 - val_binary_accuracy: 0.6831 - val_loss: 0.0785 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 2/30\n\nEpoch 2: val_loss improved from 0.07853 to 0.07537, saving model to ./BASE_DIR/models/robust_models/model_CNN_Only_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 2 ...\n[ThresholdOptimizer] Best threshold so far: 0.510 with minority F1=0.8139\n497/497 - 4s - 9ms/step - auc: 0.9049 - binary_accuracy: 0.8909 - loss: 0.0281 - precision: 0.9130 - recall: 0.9708 - val_auc: 0.6395 - val_binary_accuracy: 0.6840 - val_loss: 0.0754 - val_precision: 0.6837 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 3/30\n\nEpoch 3: val_loss improved from 0.07537 to 0.07383, saving model to ./BASE_DIR/models/robust_models/model_CNN_Only_robust.h5\n497/497 - 3s - 6ms/step - auc: 0.9120 - binary_accuracy: 0.8943 - loss: 0.0271 - precision: 0.9138 - recall: 0.9739 - val_auc: 0.7141 - val_binary_accuracy: 0.7144 - val_loss: 0.0738 - val_precision: 0.7308 - val_recall: 0.9214 - learning_rate: 0.0010\nEpoch 4/30\n\nEpoch 4: val_loss improved from 0.07383 to 0.07108, saving model to ./BASE_DIR/models/robust_models/model_CNN_Only_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 4 ...\n[ThresholdOptimizer] Best threshold so far: 0.510 with minority F1=0.8139\n497/497 - 3s - 6ms/step - auc: 0.9154 - binary_accuracy: 0.8945 - loss: 0.0266 - precision: 0.9173 - recall: 0.9696 - val_auc: 0.7141 - val_binary_accuracy: 0.6849 - val_loss: 0.0711 - val_precision: 0.6877 - val_recall: 0.9869 - learning_rate: 0.0010\nEpoch 5/30\n\nEpoch 5: val_loss improved from 0.07108 to 0.06970, saving model to ./BASE_DIR/models/robust_models/model_CNN_Only_robust.h5\n497/497 - 3s - 6ms/step - auc: 0.9191 - binary_accuracy: 0.8945 - loss: 0.0261 - precision: 0.9162 - recall: 0.9711 - val_auc: 0.7523 - val_binary_accuracy: 0.6831 - val_loss: 0.0697 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 6/30\n\nEpoch 6: val_loss improved from 0.06970 to 0.06835, saving model to ./BASE_DIR/models/robust_models/model_CNN_Only_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 6 ...\n[ThresholdOptimizer] Best threshold so far: 0.490 with minority F1=0.8248\n497/497 - 3s - 6ms/step - auc: 0.9234 - binary_accuracy: 0.8960 - loss: 0.0256 - precision: 0.9213 - recall: 0.9666 - val_auc: 0.7793 - val_binary_accuracy: 0.7252 - val_loss: 0.0683 - val_precision: 0.7562 - val_recall: 0.8820 - learning_rate: 0.0010\nEpoch 7/30\n\nEpoch 7: val_loss did not improve from 0.06835\n497/497 - 3s - 6ms/step - auc: 0.9296 - binary_accuracy: 0.8972 - loss: 0.0248 - precision: 0.9271 - recall: 0.9609 - val_auc: 0.7487 - val_binary_accuracy: 0.6938 - val_loss: 0.0718 - val_precision: 0.7709 - val_recall: 0.7851 - learning_rate: 0.0010\nEpoch 8/30\n\nEpoch 8: val_loss improved from 0.06835 to 0.06728, saving model to ./BASE_DIR/models/robust_models/model_CNN_Only_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 8 ...\n[ThresholdOptimizer] Best threshold so far: 0.490 with minority F1=0.8248\n497/497 - 3s - 6ms/step - auc: 0.9307 - binary_accuracy: 0.8993 - loss: 0.0248 - precision: 0.9310 - recall: 0.9588 - val_auc: 0.7670 - val_binary_accuracy: 0.6885 - val_loss: 0.0673 - val_precision: 0.6878 - val_recall: 0.9961 - learning_rate: 0.0010\nEpoch 9/30\n\nEpoch 9: val_loss did not improve from 0.06728\n497/497 - 3s - 6ms/step - auc: 0.9348 - binary_accuracy: 0.9022 - loss: 0.0244 - precision: 0.9298 - recall: 0.9636 - val_auc: 0.7867 - val_binary_accuracy: 0.7431 - val_loss: 0.0684 - val_precision: 0.7686 - val_recall: 0.8925 - learning_rate: 0.0010\nEpoch 10/30\n\nEpoch 10: val_loss did not improve from 0.06728\n[ThresholdOptimizer] Searching threshold at epoch 10 ...\n[ThresholdOptimizer] Best threshold so far: 0.490 with minority F1=0.8352\n497/497 - 3s - 6ms/step - auc: 0.9356 - binary_accuracy: 0.9055 - loss: 0.0247 - precision: 0.9373 - recall: 0.9587 - val_auc: 0.8029 - val_binary_accuracy: 0.7449 - val_loss: 0.0680 - val_precision: 0.8178 - val_recall: 0.8060 - learning_rate: 0.0010\nEpoch 11/30\n\nEpoch 11: val_loss did not improve from 0.06728\n\nEpoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n497/497 - 3s - 6ms/step - auc: 0.9376 - binary_accuracy: 0.9057 - loss: 0.0242 - precision: 0.9362 - recall: 0.9602 - val_auc: 0.8111 - val_binary_accuracy: 0.7216 - val_loss: 0.0715 - val_precision: 0.8818 - val_recall: 0.6841 - learning_rate: 0.0010\nEpoch 12/30\n\nEpoch 12: val_loss improved from 0.06728 to 0.06167, saving model to ./BASE_DIR/models/robust_models/model_CNN_Only_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 12 ...\n[ThresholdOptimizer] Best threshold so far: 0.500 with minority F1=0.8374\n497/497 - 3s - 6ms/step - auc: 0.9461 - binary_accuracy: 0.9114 - loss: 0.0225 - precision: 0.9457 - recall: 0.9560 - val_auc: 0.8180 - val_binary_accuracy: 0.7619 - val_loss: 0.0617 - val_precision: 0.7840 - val_recall: 0.8991 - learning_rate: 5.0000e-04\nEpoch 13/30\n\nEpoch 13: val_loss did not improve from 0.06167\n497/497 - 3s - 6ms/step - auc: 0.9493 - binary_accuracy: 0.9128 - loss: 0.0218 - precision: 0.9499 - recall: 0.9529 - val_auc: 0.8075 - val_binary_accuracy: 0.7511 - val_loss: 0.0628 - val_precision: 0.7810 - val_recall: 0.8834 - learning_rate: 5.0000e-04\nEpoch 14/30\n\nEpoch 14: val_loss improved from 0.06167 to 0.06133, saving model to ./BASE_DIR/models/robust_models/model_CNN_Only_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 14 ...\n[ThresholdOptimizer] Best threshold so far: 0.510 with minority F1=0.8413\n497/497 - 3s - 6ms/step - auc: 0.9513 - binary_accuracy: 0.9163 - loss: 0.0216 - precision: 0.9525 - recall: 0.9541 - val_auc: 0.8310 - val_binary_accuracy: 0.7520 - val_loss: 0.0613 - val_precision: 0.7553 - val_recall: 0.9423 - learning_rate: 5.0000e-04\nEpoch 15/30\n\nEpoch 15: val_loss improved from 0.06133 to 0.05988, saving model to ./BASE_DIR/models/robust_models/model_CNN_Only_robust.h5\n497/497 - 3s - 6ms/step - auc: 0.9520 - binary_accuracy: 0.9185 - loss: 0.0214 - precision: 0.9561 - recall: 0.9528 - val_auc: 0.8372 - val_binary_accuracy: 0.7681 - val_loss: 0.0599 - val_precision: 0.7930 - val_recall: 0.8938 - learning_rate: 5.0000e-04\nEpoch 16/30\n\nEpoch 16: val_loss did not improve from 0.05988\n[ThresholdOptimizer] Searching threshold at epoch 16 ...\n[ThresholdOptimizer] Best threshold so far: 0.510 with minority F1=0.8413\n497/497 - 3s - 6ms/step - auc: 0.9568 - binary_accuracy: 0.9188 - loss: 0.0201 - precision: 0.9575 - recall: 0.9517 - val_auc: 0.8200 - val_binary_accuracy: 0.7538 - val_loss: 0.0620 - val_precision: 0.7811 - val_recall: 0.8886 - learning_rate: 5.0000e-04\nEpoch 17/30\n\nEpoch 17: val_loss did not improve from 0.05988\n497/497 - 3s - 5ms/step - auc: 0.9578 - binary_accuracy: 0.9227 - loss: 0.0202 - precision: 0.9607 - recall: 0.9526 - val_auc: 0.8364 - val_binary_accuracy: 0.7708 - val_loss: 0.0622 - val_precision: 0.8412 - val_recall: 0.8191 - learning_rate: 5.0000e-04\nEpoch 18/30\n\nEpoch 18: val_loss did not improve from 0.05988\n\nEpoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n[ThresholdOptimizer] Searching threshold at epoch 18 ...\n[ThresholdOptimizer] Best threshold so far: 0.510 with minority F1=0.8413\n497/497 - 3s - 6ms/step - auc: 0.9595 - binary_accuracy: 0.9249 - loss: 0.0199 - precision: 0.9620 - recall: 0.9538 - val_auc: 0.8274 - val_binary_accuracy: 0.7628 - val_loss: 0.0624 - val_precision: 0.8007 - val_recall: 0.8689 - learning_rate: 5.0000e-04\nEpoch 19/30\n\nEpoch 19: val_loss improved from 0.05988 to 0.05865, saving model to ./BASE_DIR/models/robust_models/model_CNN_Only_robust.h5\n497/497 - 3s - 6ms/step - auc: 0.9613 - binary_accuracy: 0.9271 - loss: 0.0196 - precision: 0.9640 - recall: 0.9543 - val_auc: 0.8409 - val_binary_accuracy: 0.7771 - val_loss: 0.0586 - val_precision: 0.8286 - val_recall: 0.8493 - learning_rate: 2.5000e-04\nEpoch 20/30\n\nEpoch 20: val_loss did not improve from 0.05865\n[ThresholdOptimizer] Searching threshold at epoch 20 ...\n[ThresholdOptimizer] Best threshold so far: 0.470 with minority F1=0.8516\n497/497 - 3s - 6ms/step - auc: 0.9652 - binary_accuracy: 0.9315 - loss: 0.0187 - precision: 0.9697 - recall: 0.9533 - val_auc: 0.8463 - val_binary_accuracy: 0.7762 - val_loss: 0.0618 - val_precision: 0.8499 - val_recall: 0.8165 - learning_rate: 2.5000e-04\nEpoch 21/30\n\nEpoch 21: val_loss did not improve from 0.05865\n497/497 - 3s - 6ms/step - auc: 0.9676 - binary_accuracy: 0.9337 - loss: 0.0180 - precision: 0.9698 - recall: 0.9557 - val_auc: 0.8410 - val_binary_accuracy: 0.7708 - val_loss: 0.0677 - val_precision: 0.7924 - val_recall: 0.9004 - learning_rate: 2.5000e-04\nEpoch 22/30\n\nEpoch 22: val_loss did not improve from 0.05865\n\nEpoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n[ThresholdOptimizer] Searching threshold at epoch 22 ...\n[ThresholdOptimizer] Best threshold so far: 0.470 with minority F1=0.8559\n497/497 - 3s - 6ms/step - auc: 0.9685 - binary_accuracy: 0.9343 - loss: 0.0178 - precision: 0.9703 - recall: 0.9559 - val_auc: 0.8455 - val_binary_accuracy: 0.7699 - val_loss: 0.0602 - val_precision: 0.8475 - val_recall: 0.8087 - learning_rate: 2.5000e-04\nEpoch 23/30\n\nEpoch 23: val_loss did not improve from 0.05865\n497/497 - 3s - 5ms/step - auc: 0.9718 - binary_accuracy: 0.9384 - loss: 0.0168 - precision: 0.9727 - recall: 0.9582 - val_auc: 0.8571 - val_binary_accuracy: 0.7914 - val_loss: 0.0648 - val_precision: 0.8487 - val_recall: 0.8453 - learning_rate: 1.2500e-04\nEpoch 24/30\n\nEpoch 24: val_loss did not improve from 0.05865\n[ThresholdOptimizer] Searching threshold at epoch 24 ...\n[ThresholdOptimizer] Best threshold so far: 0.450 with minority F1=0.8559\n497/497 - 3s - 6ms/step - auc: 0.9729 - binary_accuracy: 0.9400 - loss: 0.0165 - precision: 0.9739 - recall: 0.9587 - val_auc: 0.8545 - val_binary_accuracy: 0.7878 - val_loss: 0.0591 - val_precision: 0.8564 - val_recall: 0.8283 - learning_rate: 1.2500e-04\nEpoch 25/30\n\nEpoch 25: val_loss did not improve from 0.05865\n\nEpoch 25: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n497/497 - 3s - 6ms/step - auc: 0.9718 - binary_accuracy: 0.9386 - loss: 0.0168 - precision: 0.9729 - recall: 0.9582 - val_auc: 0.8534 - val_binary_accuracy: 0.7896 - val_loss: 0.0602 - val_precision: 0.8385 - val_recall: 0.8571 - learning_rate: 1.2500e-04\nEpoch 25: early stopping\nRestoring model weights from the end of the best epoch: 19.\n[train_and_evaluate_model] Final selected threshold for CNN_Only: 0.45\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n[train_and_evaluate_model] Test metrics for CNN_Only at threshold 0.450:\n  accuracy: 0.7645\n  precision: 0.7654\n  recall: 0.9450\n  f1: 0.8457\n  minority_precision: 0.7654\n  minority_recall: 0.9450\n  minority_f1: 0.8457\n[train_and_evaluate_model] Saved metrics to ./BASE_DIR/imbalance_analysis/results_CNN_Only_robust.csv\n\n============================================================\nTRAINING MODEL: LSTM\n============================================================\n\n[train_and_evaluate_model] Starting training for 'LSTM' (aggressive=False)\n\n[Train-LSTM] Class distribution:\n  Class 0: 1650 samples\n  Class 1: 3558 samples\n  Imbalance ratio (maj/min): 2.156\n\n[Val-LSTM] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n\n[Test-LSTM] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n[train_and_evaluate_model] Target time frames for model input: 94\n[train_and_evaluate_model] Class weights used for training: {0: 1.5781818181818181, 1: 0.7318718381112985}\n[train_and_evaluate_model] Using BinaryCrossentropy loss (baseline).\n\n[Model Architecture - LSTM]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_5\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_mfcc (\u001b[38;5;33mInputLayer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ cast_6 (\u001b[38;5;33mCast\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_6 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m608,256\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_20          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_29 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_7 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m656,384\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_21          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_30 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_8 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m164,352\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_22          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_31 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_23          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_32 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_33 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ out (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_mfcc (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ cast_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">608,256</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_20          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">656,384</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_21          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_22          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_23          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,457,921\u001b[0m (5.56 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,457,921</span> (5.56 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,455,873\u001b[0m (5.55 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,455,873</span> (5.55 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,048\u001b[0m (8.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> (8.00 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"[train_and_evaluate_model] Starting model.fit for LSTM...\nEpoch 1/30\n\nEpoch 1: val_loss improved from inf to 0.69190, saving model to ./BASE_DIR/models/robust_models/model_LSTM_baseline.h5\n163/163 - 21s - 130ms/step - auc: 0.5494 - binary_accuracy: 0.5321 - loss: 0.8004 - precision: 0.7177 - recall: 0.5194 - val_auc: 0.5885 - val_binary_accuracy: 0.4575 - val_loss: 0.6919 - val_precision: 0.7962 - val_recall: 0.2765 - learning_rate: 0.0010\nEpoch 2/30\n\nEpoch 2: val_loss improved from 0.69190 to 0.63577, saving model to ./BASE_DIR/models/robust_models/model_LSTM_baseline.h5\n[ThresholdOptimizer] Searching threshold at epoch 2 ...\n[ThresholdOptimizer] Best threshold so far: 0.000 with minority F1=0.8117\n163/163 - 11s - 68ms/step - auc: 0.5534 - binary_accuracy: 0.5351 - loss: 0.7486 - precision: 0.7209 - recall: 0.5214 - val_auc: 0.5333 - val_binary_accuracy: 0.6822 - val_loss: 0.6358 - val_precision: 0.6831 - val_recall: 0.9974 - learning_rate: 0.0010\nEpoch 3/30\n\nEpoch 3: val_loss did not improve from 0.63577\n163/163 - 8s - 51ms/step - auc: 0.5780 - binary_accuracy: 0.5497 - loss: 0.7102 - precision: 0.7276 - recall: 0.5450 - val_auc: 0.5971 - val_binary_accuracy: 0.5801 - val_loss: 0.6593 - val_precision: 0.7311 - val_recall: 0.6094 - learning_rate: 0.0010\nEpoch 4/30\n\nEpoch 4: val_loss did not improve from 0.63577\n[ThresholdOptimizer] Searching threshold at epoch 4 ...\n[ThresholdOptimizer] Best threshold so far: 0.000 with minority F1=0.8117\n163/163 - 9s - 55ms/step - auc: 0.5944 - binary_accuracy: 0.5570 - loss: 0.6933 - precision: 0.7410 - recall: 0.5405 - val_auc: 0.5720 - val_binary_accuracy: 0.6580 - val_loss: 0.6409 - val_precision: 0.6830 - val_recall: 0.9318 - learning_rate: 0.0010\nEpoch 5/30\n\nEpoch 5: val_loss did not improve from 0.63577\n\nEpoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n163/163 - 8s - 51ms/step - auc: 0.6311 - binary_accuracy: 0.5835 - loss: 0.6686 - precision: 0.7777 - recall: 0.5467 - val_auc: 0.5519 - val_binary_accuracy: 0.3975 - val_loss: 0.7675 - val_precision: 0.7744 - val_recall: 0.1664 - learning_rate: 0.0010\nEpoch 6/30\n\nEpoch 6: val_loss did not improve from 0.63577\n[ThresholdOptimizer] Searching threshold at epoch 6 ...\n[ThresholdOptimizer] Best threshold so far: 0.000 with minority F1=0.8117\n163/163 - 9s - 55ms/step - auc: 0.6771 - binary_accuracy: 0.6102 - loss: 0.6413 - precision: 0.7970 - recall: 0.5762 - val_auc: 0.6216 - val_binary_accuracy: 0.3679 - val_loss: 0.7680 - val_precision: 0.8519 - val_recall: 0.0904 - learning_rate: 5.0000e-04\nEpoch 7/30\n\nEpoch 7: val_loss improved from 0.63577 to 0.63118, saving model to ./BASE_DIR/models/robust_models/model_LSTM_baseline.h5\n163/163 - 8s - 52ms/step - auc: 0.7019 - binary_accuracy: 0.6269 - loss: 0.6278 - precision: 0.8163 - recall: 0.5857 - val_auc: 0.6406 - val_binary_accuracy: 0.6383 - val_loss: 0.6312 - val_precision: 0.7503 - val_recall: 0.7051 - learning_rate: 5.0000e-04\nEpoch 8/30\n\nEpoch 8: val_loss improved from 0.63118 to 0.59622, saving model to ./BASE_DIR/models/robust_models/model_LSTM_baseline.h5\n[ThresholdOptimizer] Searching threshold at epoch 8 ...\n[ThresholdOptimizer] Best threshold so far: 0.460 with minority F1=0.8148\n163/163 - 9s - 56ms/step - auc: 0.7147 - binary_accuracy: 0.6361 - loss: 0.6178 - precision: 0.8288 - recall: 0.5891 - val_auc: 0.6411 - val_binary_accuracy: 0.6947 - val_loss: 0.5962 - val_precision: 0.7085 - val_recall: 0.9397 - learning_rate: 5.0000e-04\nEpoch 9/30\n\nEpoch 9: val_loss improved from 0.59622 to 0.54878, saving model to ./BASE_DIR/models/robust_models/model_LSTM_baseline.h5\n163/163 - 8s - 52ms/step - auc: 0.7335 - binary_accuracy: 0.6540 - loss: 0.6037 - precision: 0.8414 - recall: 0.6082 - val_auc: 0.7564 - val_binary_accuracy: 0.7189 - val_loss: 0.5488 - val_precision: 0.7583 - val_recall: 0.8637 - learning_rate: 5.0000e-04\nEpoch 10/30\n\nEpoch 10: val_loss did not improve from 0.54878\n[ThresholdOptimizer] Searching threshold at epoch 10 ...\n[ThresholdOptimizer] Best threshold so far: 0.460 with minority F1=0.8148\n163/163 - 9s - 54ms/step - auc: 0.7588 - binary_accuracy: 0.6730 - loss: 0.5859 - precision: 0.8594 - recall: 0.6234 - val_auc: 0.6484 - val_binary_accuracy: 0.5515 - val_loss: 0.6819 - val_precision: 0.8149 - val_recall: 0.4443 - learning_rate: 5.0000e-04\nEpoch 11/30\n\nEpoch 11: val_loss did not improve from 0.54878\n163/163 - 8s - 50ms/step - auc: 0.7733 - binary_accuracy: 0.6880 - loss: 0.5668 - precision: 0.8730 - recall: 0.6358 - val_auc: 0.7083 - val_binary_accuracy: 0.6688 - val_loss: 0.5998 - val_precision: 0.7946 - val_recall: 0.6946 - learning_rate: 5.0000e-04\nEpoch 12/30\n\nEpoch 12: val_loss improved from 0.54878 to 0.50328, saving model to ./BASE_DIR/models/robust_models/model_LSTM_baseline.h5\n[ThresholdOptimizer] Searching threshold at epoch 12 ...\n[ThresholdOptimizer] Best threshold so far: 0.520 with minority F1=0.8335\n163/163 - 9s - 54ms/step - auc: 0.7756 - binary_accuracy: 0.6836 - loss: 0.5643 - precision: 0.8628 - recall: 0.6383 - val_auc: 0.7981 - val_binary_accuracy: 0.7511 - val_loss: 0.5033 - val_precision: 0.7823 - val_recall: 0.8807 - learning_rate: 5.0000e-04\nEpoch 13/30\n\nEpoch 13: val_loss did not improve from 0.50328\n163/163 - 8s - 50ms/step - auc: 0.7825 - binary_accuracy: 0.6937 - loss: 0.5575 - precision: 0.8756 - recall: 0.6431 - val_auc: 0.6045 - val_binary_accuracy: 0.6858 - val_loss: 0.7082 - val_precision: 0.6849 - val_recall: 1.0000 - learning_rate: 5.0000e-04\nEpoch 14/30\n\nEpoch 14: val_loss did not improve from 0.50328\n[ThresholdOptimizer] Searching threshold at epoch 14 ...\n[ThresholdOptimizer] Best threshold so far: 0.520 with minority F1=0.8335\n163/163 - 9s - 53ms/step - auc: 0.7845 - binary_accuracy: 0.6941 - loss: 0.5571 - precision: 0.8692 - recall: 0.6501 - val_auc: 0.6942 - val_binary_accuracy: 0.6983 - val_loss: 0.5745 - val_precision: 0.7367 - val_recall: 0.8689 - learning_rate: 5.0000e-04\nEpoch 15/30\n\nEpoch 15: val_loss did not improve from 0.50328\n\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n163/163 - 8s - 50ms/step - auc: 0.8057 - binary_accuracy: 0.7183 - loss: 0.5332 - precision: 0.8851 - recall: 0.6754 - val_auc: 0.6753 - val_binary_accuracy: 0.6132 - val_loss: 0.7336 - val_precision: 0.7782 - val_recall: 0.6068 - learning_rate: 5.0000e-04\nEpoch 16/30\n\nEpoch 16: val_loss did not improve from 0.50328\n[ThresholdOptimizer] Searching threshold at epoch 16 ...\n[ThresholdOptimizer] Best threshold so far: 0.240 with minority F1=0.8431\n163/163 - 9s - 53ms/step - auc: 0.8193 - binary_accuracy: 0.7287 - loss: 0.5148 - precision: 0.8977 - recall: 0.6804 - val_auc: 0.8185 - val_binary_accuracy: 0.6929 - val_loss: 0.5530 - val_precision: 0.9054 - val_recall: 0.6147 - learning_rate: 2.5000e-04\nEpoch 17/30\n\nEpoch 17: val_loss did not improve from 0.50328\n163/163 - 8s - 50ms/step - auc: 0.8399 - binary_accuracy: 0.7469 - loss: 0.4910 - precision: 0.9061 - recall: 0.7024 - val_auc: 0.7974 - val_binary_accuracy: 0.6571 - val_loss: 0.6170 - val_precision: 0.9148 - val_recall: 0.5491 - learning_rate: 2.5000e-04\nEpoch 18/30\n\nEpoch 18: val_loss did not improve from 0.50328\n\nEpoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n[ThresholdOptimizer] Searching threshold at epoch 18 ...\n[ThresholdOptimizer] Best threshold so far: 0.240 with minority F1=0.8431\n163/163 - 9s - 53ms/step - auc: 0.8503 - binary_accuracy: 0.7558 - loss: 0.4718 - precision: 0.9156 - recall: 0.7077 - val_auc: 0.8253 - val_binary_accuracy: 0.6741 - val_loss: 0.6046 - val_precision: 0.9200 - val_recall: 0.5727 - learning_rate: 2.5000e-04\nEpoch 18: early stopping\nRestoring model weights from the end of the best epoch: 12.\n[train_and_evaluate_model] Final selected threshold for LSTM: 0.24\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n[train_and_evaluate_model] Test metrics for LSTM at threshold 0.240:\n  accuracy: 0.6831\n  precision: 0.6831\n  recall: 1.0000\n  f1: 0.8117\n  minority_precision: 0.6831\n  minority_recall: 1.0000\n  minority_f1: 0.8117\n[train_and_evaluate_model] Saved metrics to ./BASE_DIR/imbalance_analysis/results_LSTM_baseline.csv\n\n[train_and_evaluate_model] Starting training for 'LSTM' (aggressive=True)\n\n[Train-LSTM] Class distribution:\n  Class 0: 1650 samples\n  Class 1: 3558 samples\n  Imbalance ratio (maj/min): 2.156\n\n[Val-LSTM] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n\n[Test-LSTM] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n[train_and_evaluate_model] Target time frames for model input: 94\n[train_and_evaluate_model] Applying heavy spec_augment to minority samples for LSTM...\n[train_and_evaluate_model] Created 10674 augmented MFCC samples for minority.\n[train_and_evaluate_model] New training size: 15882\n[train_and_evaluate_model] Class weights used for training: {0: 4.8127272727272725, 1: 0.5579679595278246}\n[train_and_evaluate_model] Using dynamic focal loss with alpha_pos=0.5000\n\n[Model Architecture - LSTM]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_6\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_mfcc (\u001b[38;5;33mInputLayer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ cast_7 (\u001b[38;5;33mCast\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_9 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m608,256\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_24          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_34 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_10                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m656,384\u001b[0m │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_25          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_35 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_11                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m164,352\u001b[0m │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_26          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_36 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_27          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_37 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_38 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ out (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_mfcc (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ cast_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">608,256</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_24          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_10                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">656,384</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_25          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_11                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_26          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_27          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,457,921\u001b[0m (5.56 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,457,921</span> (5.56 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,455,873\u001b[0m (5.55 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,455,873</span> (5.55 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,048\u001b[0m (8.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> (8.00 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"[train_and_evaluate_model] Starting model.fit for LSTM...\nEpoch 1/30\n\nEpoch 1: val_loss improved from inf to 0.18211, saving model to ./BASE_DIR/models/robust_models/model_LSTM_robust.h5\n497/497 - 35s - 71ms/step - auc: 0.6094 - binary_accuracy: 0.8160 - loss: 0.0801 - precision: 0.9012 - recall: 0.8926 - val_auc: 0.5871 - val_binary_accuracy: 0.6831 - val_loss: 0.1821 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 2/30\n\nEpoch 2: val_loss improved from 0.18211 to 0.07984, saving model to ./BASE_DIR/models/robust_models/model_LSTM_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 2 ...\n[ThresholdOptimizer] Best threshold so far: 0.000 with minority F1=0.8117\n497/497 - 26s - 52ms/step - auc: 0.8054 - binary_accuracy: 0.8804 - loss: 0.0401 - precision: 0.9039 - recall: 0.9696 - val_auc: 0.4437 - val_binary_accuracy: 0.6831 - val_loss: 0.0798 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 3/30\n\nEpoch 3: val_loss improved from 0.07984 to 0.07694, saving model to ./BASE_DIR/models/robust_models/model_LSTM_robust.h5\n497/497 - 23s - 46ms/step - auc: 0.8537 - binary_accuracy: 0.8909 - loss: 0.0338 - precision: 0.9004 - recall: 0.9875 - val_auc: 0.5509 - val_binary_accuracy: 0.6831 - val_loss: 0.0769 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 4/30\n\nEpoch 4: val_loss did not improve from 0.07694\n[ThresholdOptimizer] Searching threshold at epoch 4 ...\n[ThresholdOptimizer] Best threshold so far: 0.000 with minority F1=0.8117\n497/497 - 24s - 47ms/step - auc: 0.8760 - binary_accuracy: 0.8954 - loss: 0.0316 - precision: 0.8980 - recall: 0.9965 - val_auc: 0.5850 - val_binary_accuracy: 0.6831 - val_loss: 0.5288 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 5/30\n\nEpoch 5: val_loss did not improve from 0.07694\n497/497 - 23s - 46ms/step - auc: 0.8800 - binary_accuracy: 0.8957 - loss: 0.0304 - precision: 0.8965 - recall: 0.9989 - val_auc: 0.6695 - val_binary_accuracy: 0.6831 - val_loss: 0.1816 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 6/30\n\nEpoch 6: val_loss did not improve from 0.07694\n\nEpoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n[ThresholdOptimizer] Searching threshold at epoch 6 ...\n[ThresholdOptimizer] Best threshold so far: 0.000 with minority F1=0.8117\n497/497 - 24s - 48ms/step - auc: 0.8868 - binary_accuracy: 0.8962 - loss: 0.0295 - precision: 0.8973 - recall: 0.9985 - val_auc: 0.5015 - val_binary_accuracy: 0.6831 - val_loss: 0.8989 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 7/30\n\nEpoch 7: val_loss improved from 0.07694 to 0.07618, saving model to ./BASE_DIR/models/robust_models/model_LSTM_robust.h5\n497/497 - 24s - 49ms/step - auc: 0.9006 - binary_accuracy: 0.8952 - loss: 0.0281 - precision: 0.9012 - recall: 0.9917 - val_auc: 0.6523 - val_binary_accuracy: 0.6885 - val_loss: 0.0762 - val_precision: 0.6909 - val_recall: 0.9843 - learning_rate: 5.0000e-04\nEpoch 8/30\n\nEpoch 8: val_loss improved from 0.07618 to 0.07257, saving model to ./BASE_DIR/models/robust_models/model_LSTM_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 8 ...\n[ThresholdOptimizer] Best threshold so far: 0.510 with minority F1=0.8185\n497/497 - 25s - 50ms/step - auc: 0.9068 - binary_accuracy: 0.8950 - loss: 0.0277 - precision: 0.9059 - recall: 0.9852 - val_auc: 0.7205 - val_binary_accuracy: 0.6947 - val_loss: 0.0726 - val_precision: 0.6932 - val_recall: 0.9921 - learning_rate: 5.0000e-04\nEpoch 9/30\n\nEpoch 9: val_loss improved from 0.07257 to 0.07224, saving model to ./BASE_DIR/models/robust_models/model_LSTM_robust.h5\n497/497 - 24s - 48ms/step - auc: 0.9093 - binary_accuracy: 0.8955 - loss: 0.0277 - precision: 0.9058 - recall: 0.9860 - val_auc: 0.7484 - val_binary_accuracy: 0.7037 - val_loss: 0.0722 - val_precision: 0.7073 - val_recall: 0.9659 - learning_rate: 5.0000e-04\nEpoch 10/30\n\nEpoch 10: val_loss did not improve from 0.07224\n[ThresholdOptimizer] Searching threshold at epoch 10 ...\n[ThresholdOptimizer] Best threshold so far: 0.510 with minority F1=0.8185\n497/497 - 25s - 50ms/step - auc: 0.9194 - binary_accuracy: 0.8967 - loss: 0.0262 - precision: 0.9157 - recall: 0.9745 - val_auc: 0.5251 - val_binary_accuracy: 0.6428 - val_loss: 0.0856 - val_precision: 0.6802 - val_recall: 0.9004 - learning_rate: 5.0000e-04\nEpoch 11/30\n\nEpoch 11: val_loss improved from 0.07224 to 0.07117, saving model to ./BASE_DIR/models/robust_models/model_LSTM_robust.h5\n497/497 - 24s - 49ms/step - auc: 0.9221 - binary_accuracy: 0.8989 - loss: 0.0266 - precision: 0.9227 - recall: 0.9683 - val_auc: 0.7517 - val_binary_accuracy: 0.7064 - val_loss: 0.0712 - val_precision: 0.8068 - val_recall: 0.7497 - learning_rate: 5.0000e-04\nEpoch 12/30\n\nEpoch 12: val_loss improved from 0.07117 to 0.06644, saving model to ./BASE_DIR/models/robust_models/model_LSTM_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 12 ...\n[ThresholdOptimizer] Best threshold so far: 0.520 with minority F1=0.8237\n497/497 - 25s - 50ms/step - auc: 0.9279 - binary_accuracy: 0.9010 - loss: 0.0256 - precision: 0.9262 - recall: 0.9666 - val_auc: 0.7764 - val_binary_accuracy: 0.7090 - val_loss: 0.0664 - val_precision: 0.7143 - val_recall: 0.9567 - learning_rate: 5.0000e-04\nEpoch 13/30\n\nEpoch 13: val_loss did not improve from 0.06644\n497/497 - 24s - 48ms/step - auc: 0.9306 - binary_accuracy: 0.9009 - loss: 0.0251 - precision: 0.9295 - recall: 0.9624 - val_auc: 0.8003 - val_binary_accuracy: 0.7574 - val_loss: 0.0676 - val_precision: 0.8203 - val_recall: 0.8257 - learning_rate: 5.0000e-04\nEpoch 14/30\n\nEpoch 14: val_loss improved from 0.06644 to 0.06379, saving model to ./BASE_DIR/models/robust_models/model_LSTM_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 14 ...\n[ThresholdOptimizer] Best threshold so far: 0.490 with minority F1=0.8330\n497/497 - 25s - 50ms/step - auc: 0.9337 - binary_accuracy: 0.9049 - loss: 0.0245 - precision: 0.9342 - recall: 0.9616 - val_auc: 0.8097 - val_binary_accuracy: 0.7493 - val_loss: 0.0638 - val_precision: 0.7704 - val_recall: 0.9017 - learning_rate: 5.0000e-04\nEpoch 15/30\n\nEpoch 15: val_loss did not improve from 0.06379\n497/497 - 24s - 48ms/step - auc: 0.9332 - binary_accuracy: 0.9024 - loss: 0.0245 - precision: 0.9347 - recall: 0.9581 - val_auc: 0.7172 - val_binary_accuracy: 0.6607 - val_loss: 0.0742 - val_precision: 0.8310 - val_recall: 0.6317 - learning_rate: 5.0000e-04\nEpoch 16/30\n\nEpoch 16: val_loss did not improve from 0.06379\n[ThresholdOptimizer] Searching threshold at epoch 16 ...\n[ThresholdOptimizer] Best threshold so far: 0.490 with minority F1=0.8330\n497/497 - 25s - 49ms/step - auc: 0.9351 - binary_accuracy: 0.9021 - loss: 0.0243 - precision: 0.9335 - recall: 0.9591 - val_auc: 0.6383 - val_binary_accuracy: 0.6768 - val_loss: 0.0881 - val_precision: 0.6882 - val_recall: 0.9633 - learning_rate: 5.0000e-04\nEpoch 17/30\n\nEpoch 17: val_loss did not improve from 0.06379\n\nEpoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n497/497 - 24s - 48ms/step - auc: 0.9353 - binary_accuracy: 0.9020 - loss: 0.0241 - precision: 0.9342 - recall: 0.9581 - val_auc: 0.8000 - val_binary_accuracy: 0.6902 - val_loss: 0.0774 - val_precision: 0.6887 - val_recall: 0.9974 - learning_rate: 5.0000e-04\nEpoch 18/30\n\nEpoch 18: val_loss improved from 0.06379 to 0.06218, saving model to ./BASE_DIR/models/robust_models/model_LSTM_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 18 ...\n[ThresholdOptimizer] Best threshold so far: 0.480 with minority F1=0.8430\n497/497 - 25s - 50ms/step - auc: 0.9454 - binary_accuracy: 0.9107 - loss: 0.0225 - precision: 0.9481 - recall: 0.9525 - val_auc: 0.8215 - val_binary_accuracy: 0.7610 - val_loss: 0.0622 - val_precision: 0.8307 - val_recall: 0.8165 - learning_rate: 2.5000e-04\nEpoch 19/30\n\nEpoch 19: val_loss improved from 0.06218 to 0.06140, saving model to ./BASE_DIR/models/robust_models/model_LSTM_robust.h5\n497/497 - 24s - 49ms/step - auc: 0.9496 - binary_accuracy: 0.9141 - loss: 0.0217 - precision: 0.9532 - recall: 0.9508 - val_auc: 0.8288 - val_binary_accuracy: 0.7735 - val_loss: 0.0614 - val_precision: 0.8522 - val_recall: 0.8087 - learning_rate: 2.5000e-04\nEpoch 20/30\n\nEpoch 20: val_loss did not improve from 0.06140\n[ThresholdOptimizer] Searching threshold at epoch 20 ...\n[ThresholdOptimizer] Best threshold so far: 0.500 with minority F1=0.8464\n497/497 - 25s - 49ms/step - auc: 0.9511 - binary_accuracy: 0.9172 - loss: 0.0218 - precision: 0.9538 - recall: 0.9538 - val_auc: 0.8253 - val_binary_accuracy: 0.7789 - val_loss: 0.0629 - val_precision: 0.8014 - val_recall: 0.8991 - learning_rate: 2.5000e-04\nEpoch 21/30\n\nEpoch 21: val_loss improved from 0.06140 to 0.06136, saving model to ./BASE_DIR/models/robust_models/model_LSTM_robust.h5\n497/497 - 24s - 48ms/step - auc: 0.9541 - binary_accuracy: 0.9200 - loss: 0.0208 - precision: 0.9589 - recall: 0.9515 - val_auc: 0.8297 - val_binary_accuracy: 0.7610 - val_loss: 0.0614 - val_precision: 0.8473 - val_recall: 0.7929 - learning_rate: 2.5000e-04\nEpoch 22/30\n\nEpoch 22: val_loss did not improve from 0.06136\n\nEpoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n[ThresholdOptimizer] Searching threshold at epoch 22 ...\n[ThresholdOptimizer] Best threshold so far: 0.440 with minority F1=0.8501\n497/497 - 25s - 49ms/step - auc: 0.9565 - binary_accuracy: 0.9201 - loss: 0.0205 - precision: 0.9595 - recall: 0.9510 - val_auc: 0.8425 - val_binary_accuracy: 0.7368 - val_loss: 0.0633 - val_precision: 0.9050 - val_recall: 0.6868 - learning_rate: 2.5000e-04\nEpoch 23/30\n\nEpoch 23: val_loss improved from 0.06136 to 0.06029, saving model to ./BASE_DIR/models/robust_models/model_LSTM_robust.h5\n497/497 - 24s - 48ms/step - auc: 0.9618 - binary_accuracy: 0.9265 - loss: 0.0193 - precision: 0.9660 - recall: 0.9515 - val_auc: 0.8421 - val_binary_accuracy: 0.7807 - val_loss: 0.0603 - val_precision: 0.8700 - val_recall: 0.7982 - learning_rate: 1.2500e-04\nEpoch 24/30\n\nEpoch 24: val_loss did not improve from 0.06029\n[ThresholdOptimizer] Searching threshold at epoch 24 ...\n[ThresholdOptimizer] Best threshold so far: 0.440 with minority F1=0.8501\n497/497 - 25s - 49ms/step - auc: 0.9642 - binary_accuracy: 0.9275 - loss: 0.0184 - precision: 0.9662 - recall: 0.9524 - val_auc: 0.8443 - val_binary_accuracy: 0.7663 - val_loss: 0.0626 - val_precision: 0.8757 - val_recall: 0.7667 - learning_rate: 1.2500e-04\nEpoch 25/30\n\nEpoch 25: val_loss did not improve from 0.06029\n497/497 - 24s - 48ms/step - auc: 0.9659 - binary_accuracy: 0.9292 - loss: 0.0180 - precision: 0.9689 - recall: 0.9516 - val_auc: 0.8384 - val_binary_accuracy: 0.7726 - val_loss: 0.0639 - val_precision: 0.8793 - val_recall: 0.7733 - learning_rate: 1.2500e-04\nEpoch 26/30\n\nEpoch 26: val_loss did not improve from 0.06029\n\nEpoch 26: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n[ThresholdOptimizer] Searching threshold at epoch 26 ...\n[ThresholdOptimizer] Best threshold so far: 0.440 with minority F1=0.8501\n497/497 - 25s - 49ms/step - auc: 0.9694 - binary_accuracy: 0.9337 - loss: 0.0170 - precision: 0.9736 - recall: 0.9519 - val_auc: 0.8312 - val_binary_accuracy: 0.7789 - val_loss: 0.0705 - val_precision: 0.8496 - val_recall: 0.8218 - learning_rate: 1.2500e-04\nEpoch 27/30\n\nEpoch 27: val_loss did not improve from 0.06029\n497/497 - 24s - 48ms/step - auc: 0.9736 - binary_accuracy: 0.9384 - loss: 0.0158 - precision: 0.9754 - recall: 0.9553 - val_auc: 0.8503 - val_binary_accuracy: 0.7825 - val_loss: 0.0670 - val_precision: 0.8746 - val_recall: 0.7955 - learning_rate: 6.2500e-05\nEpoch 28/30\n\nEpoch 28: val_loss did not improve from 0.06029\n[ThresholdOptimizer] Searching threshold at epoch 28 ...\n[ThresholdOptimizer] Best threshold so far: 0.410 with minority F1=0.8592\n497/497 - 25s - 50ms/step - auc: 0.9751 - binary_accuracy: 0.9453 - loss: 0.0154 - precision: 0.9808 - recall: 0.9578 - val_auc: 0.8465 - val_binary_accuracy: 0.7753 - val_loss: 0.0686 - val_precision: 0.8765 - val_recall: 0.7811 - learning_rate: 6.2500e-05\nEpoch 29/30\n\nEpoch 29: val_loss did not improve from 0.06029\n\nEpoch 29: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n497/497 - 24s - 48ms/step - auc: 0.9760 - binary_accuracy: 0.9431 - loss: 0.0151 - precision: 0.9783 - recall: 0.9578 - val_auc: 0.8403 - val_binary_accuracy: 0.7574 - val_loss: 0.0695 - val_precision: 0.8880 - val_recall: 0.7379 - learning_rate: 6.2500e-05\nEpoch 29: early stopping\nRestoring model weights from the end of the best epoch: 23.\n[train_and_evaluate_model] Final selected threshold for LSTM: 0.41000000000000003\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n[train_and_evaluate_model] Test metrics for LSTM at threshold 0.410:\n  accuracy: 0.7422\n  precision: 0.7345\n  recall: 0.9751\n  f1: 0.8378\n  minority_precision: 0.7345\n  minority_recall: 0.9751\n  minority_f1: 0.8378\n[train_and_evaluate_model] Saved metrics to ./BASE_DIR/imbalance_analysis/results_LSTM_robust.csv\n\n============================================================\nTRAINING MODEL: Transformer\n============================================================\n\n[train_and_evaluate_model] Starting training for 'Transformer' (aggressive=False)\n\n[Train-Transformer] Class distribution:\n  Class 0: 1650 samples\n  Class 1: 3558 samples\n  Imbalance ratio (maj/min): 2.156\n\n[Val-Transformer] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n\n[Test-Transformer] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n[train_and_evaluate_model] Target time frames for model input: 94\n[train_and_evaluate_model] Class weights used for training: {0: 1.5781818181818181, 1: 0.7318718381112985}\n[train_and_evaluate_model] Using BinaryCrossentropy loss (baseline).\n\n[Model Architecture - Transformer]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_9\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_mfcc          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │         \u001b[38;5;34m80\u001b[0m │ input_mfcc[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │     \u001b[38;5;34m41,768\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │         \u001b[38;5;34m80\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ sequential          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │     \u001b[38;5;34m10,408\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ sequential[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │         \u001b[38;5;34m80\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │     \u001b[38;5;34m41,768\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │         \u001b[38;5;34m80\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ sequential_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │     \u001b[38;5;34m10,408\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ sequential_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │         \u001b[38;5;34m80\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_41          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_20 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m2,624\u001b[0m │ dropout_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_42          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ out (\u001b[38;5;33mDense\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_mfcc          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │ input_mfcc[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,768</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ sequential          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,408</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,768</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ sequential_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,408</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ sequential_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_41          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,624</span> │ dropout_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_42          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m107,441\u001b[0m (419.69 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">107,441</span> (419.69 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m107,441\u001b[0m (419.69 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">107,441</span> (419.69 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"[train_and_evaluate_model] Starting model.fit for Transformer...\nEpoch 1/30\n\nEpoch 1: val_loss improved from inf to 0.70046, saving model to ./BASE_DIR/models/robust_models/model_Transformer_baseline.h5\n163/163 - 32s - 199ms/step - auc: 0.5157 - binary_accuracy: 0.5156 - loss: 0.7084 - precision: 0.6848 - recall: 0.5391 - val_auc: 0.6216 - val_binary_accuracy: 0.3948 - val_loss: 0.7005 - val_precision: 0.9143 - val_recall: 0.1258 - learning_rate: 0.0010\nEpoch 2/30\n\nEpoch 2: val_loss improved from 0.70046 to 0.66005, saving model to ./BASE_DIR/models/robust_models/model_Transformer_baseline.h5\n[ThresholdOptimizer] Searching threshold at epoch 2 ...\n[ThresholdOptimizer] Best threshold so far: 0.440 with minority F1=0.8121\n163/163 - 4s - 23ms/step - auc: 0.5551 - binary_accuracy: 0.4979 - loss: 0.6828 - precision: 0.7211 - recall: 0.4323 - val_auc: 0.6720 - val_binary_accuracy: 0.4217 - val_loss: 0.6600 - val_precision: 0.8824 - val_recall: 0.1769 - learning_rate: 0.0010\nEpoch 3/30\n\nEpoch 3: val_loss improved from 0.66005 to 0.62631, saving model to ./BASE_DIR/models/robust_models/model_Transformer_baseline.h5\n163/163 - 1s - 8ms/step - auc: 0.6255 - binary_accuracy: 0.5682 - loss: 0.6664 - precision: 0.7574 - recall: 0.5413 - val_auc: 0.6917 - val_binary_accuracy: 0.6061 - val_loss: 0.6263 - val_precision: 0.8112 - val_recall: 0.5518 - learning_rate: 0.0010\nEpoch 4/30\n\nEpoch 4: val_loss improved from 0.62631 to 0.59904, saving model to ./BASE_DIR/models/robust_models/model_Transformer_baseline.h5\n[ThresholdOptimizer] Searching threshold at epoch 4 ...\n[ThresholdOptimizer] Best threshold so far: 0.390 with minority F1=0.8207\n163/163 - 2s - 10ms/step - auc: 0.6814 - binary_accuracy: 0.6133 - loss: 0.6381 - precision: 0.7978 - recall: 0.5812 - val_auc: 0.7255 - val_binary_accuracy: 0.6643 - val_loss: 0.5990 - val_precision: 0.8099 - val_recall: 0.6645 - learning_rate: 0.0010\nEpoch 5/30\n\nEpoch 5: val_loss improved from 0.59904 to 0.59174, saving model to ./BASE_DIR/models/robust_models/model_Transformer_baseline.h5\n163/163 - 1s - 8ms/step - auc: 0.7009 - binary_accuracy: 0.6308 - loss: 0.6270 - precision: 0.8195 - recall: 0.5894 - val_auc: 0.7352 - val_binary_accuracy: 0.6741 - val_loss: 0.5917 - val_precision: 0.8009 - val_recall: 0.6959 - learning_rate: 0.0010\nEpoch 6/30\n\nEpoch 6: val_loss did not improve from 0.59174\n[ThresholdOptimizer] Searching threshold at epoch 6 ...\n[ThresholdOptimizer] Best threshold so far: 0.320 with minority F1=0.8209\n163/163 - 1s - 9ms/step - auc: 0.7271 - binary_accuracy: 0.6454 - loss: 0.6059 - precision: 0.8423 - recall: 0.5916 - val_auc: 0.7357 - val_binary_accuracy: 0.6401 - val_loss: 0.6093 - val_precision: 0.8386 - val_recall: 0.5858 - learning_rate: 0.0010\nEpoch 7/30\n\nEpoch 7: val_loss did not improve from 0.59174\n163/163 - 1s - 8ms/step - auc: 0.7299 - binary_accuracy: 0.6559 - loss: 0.6053 - precision: 0.8396 - recall: 0.6135 - val_auc: 0.7309 - val_binary_accuracy: 0.6374 - val_loss: 0.5928 - val_precision: 0.8208 - val_recall: 0.6003 - learning_rate: 0.0010\nEpoch 8/30\n\nEpoch 8: val_loss improved from 0.59174 to 0.58805, saving model to ./BASE_DIR/models/robust_models/model_Transformer_baseline.h5\n[ThresholdOptimizer] Searching threshold at epoch 8 ...\n[ThresholdOptimizer] Best threshold so far: 0.320 with minority F1=0.8209\n163/163 - 2s - 10ms/step - auc: 0.7497 - binary_accuracy: 0.6713 - loss: 0.5874 - precision: 0.8528 - recall: 0.6270 - val_auc: 0.7296 - val_binary_accuracy: 0.6634 - val_loss: 0.5881 - val_precision: 0.8177 - val_recall: 0.6527 - learning_rate: 0.0010\nEpoch 9/30\n\nEpoch 9: val_loss did not improve from 0.58805\n163/163 - 1s - 8ms/step - auc: 0.7546 - binary_accuracy: 0.6759 - loss: 0.5833 - precision: 0.8520 - recall: 0.6360 - val_auc: 0.7364 - val_binary_accuracy: 0.6544 - val_loss: 0.6022 - val_precision: 0.8577 - val_recall: 0.5924 - learning_rate: 0.0010\nEpoch 10/30\n\nEpoch 10: val_loss did not improve from 0.58805\n[ThresholdOptimizer] Searching threshold at epoch 10 ...\n[ThresholdOptimizer] Best threshold so far: 0.320 with minority F1=0.8209\n163/163 - 2s - 9ms/step - auc: 0.7666 - binary_accuracy: 0.6876 - loss: 0.5724 - precision: 0.8687 - recall: 0.6394 - val_auc: 0.7499 - val_binary_accuracy: 0.6437 - val_loss: 0.5960 - val_precision: 0.8643 - val_recall: 0.5675 - learning_rate: 0.0010\nEpoch 11/30\n\nEpoch 11: val_loss improved from 0.58805 to 0.58690, saving model to ./BASE_DIR/models/robust_models/model_Transformer_baseline.h5\n163/163 - 1s - 8ms/step - auc: 0.7720 - binary_accuracy: 0.6926 - loss: 0.5672 - precision: 0.8669 - recall: 0.6498 - val_auc: 0.7468 - val_binary_accuracy: 0.6670 - val_loss: 0.5869 - val_precision: 0.8600 - val_recall: 0.6121 - learning_rate: 0.0010\nEpoch 12/30\n\nEpoch 12: val_loss did not improve from 0.58690\n[ThresholdOptimizer] Searching threshold at epoch 12 ...\n[ThresholdOptimizer] Best threshold so far: 0.320 with minority F1=0.8209\n163/163 - 2s - 9ms/step - auc: 0.7809 - binary_accuracy: 0.6993 - loss: 0.5553 - precision: 0.8689 - recall: 0.6594 - val_auc: 0.7396 - val_binary_accuracy: 0.6419 - val_loss: 0.6125 - val_precision: 0.8652 - val_recall: 0.5636 - learning_rate: 0.0010\nEpoch 13/30\n\nEpoch 13: val_loss did not improve from 0.58690\n163/163 - 1s - 8ms/step - auc: 0.7862 - binary_accuracy: 0.7060 - loss: 0.5521 - precision: 0.8684 - recall: 0.6714 - val_auc: 0.7407 - val_binary_accuracy: 0.6455 - val_loss: 0.6377 - val_precision: 0.8605 - val_recall: 0.5740 - learning_rate: 0.0010\nEpoch 14/30\n\nEpoch 14: val_loss improved from 0.58690 to 0.57500, saving model to ./BASE_DIR/models/robust_models/model_Transformer_baseline.h5\n[ThresholdOptimizer] Searching threshold at epoch 14 ...\n[ThresholdOptimizer] Best threshold so far: 0.320 with minority F1=0.8209\n163/163 - 2s - 10ms/step - auc: 0.7856 - binary_accuracy: 0.7079 - loss: 0.5570 - precision: 0.8705 - recall: 0.6726 - val_auc: 0.7458 - val_binary_accuracy: 0.6705 - val_loss: 0.5750 - val_precision: 0.8297 - val_recall: 0.6514 - learning_rate: 0.0010\nEpoch 15/30\n\nEpoch 15: val_loss did not improve from 0.57500\n163/163 - 1s - 8ms/step - auc: 0.7875 - binary_accuracy: 0.7204 - loss: 0.5523 - precision: 0.8644 - recall: 0.7007 - val_auc: 0.7447 - val_binary_accuracy: 0.6464 - val_loss: 0.6446 - val_precision: 0.8622 - val_recall: 0.5740 - learning_rate: 0.0010\nEpoch 16/30\n\nEpoch 16: val_loss did not improve from 0.57500\n[ThresholdOptimizer] Searching threshold at epoch 16 ...\n[ThresholdOptimizer] Best threshold so far: 0.320 with minority F1=0.8209\n163/163 - 1s - 9ms/step - auc: 0.8015 - binary_accuracy: 0.7247 - loss: 0.5347 - precision: 0.8750 - recall: 0.6965 - val_auc: 0.7396 - val_binary_accuracy: 0.6303 - val_loss: 0.6439 - val_precision: 0.8601 - val_recall: 0.5478 - learning_rate: 0.0010\nEpoch 17/30\n\nEpoch 17: val_loss did not improve from 0.57500\n\nEpoch 17: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n163/163 - 1s - 8ms/step - auc: 0.8066 - binary_accuracy: 0.7273 - loss: 0.5293 - precision: 0.8761 - recall: 0.6998 - val_auc: 0.7411 - val_binary_accuracy: 0.6491 - val_loss: 0.6585 - val_precision: 0.8659 - val_recall: 0.5754 - learning_rate: 0.0010\nEpoch 18/30\n\nEpoch 18: val_loss did not improve from 0.57500\n[ThresholdOptimizer] Searching threshold at epoch 18 ...\n[ThresholdOptimizer] Best threshold so far: 0.320 with minority F1=0.8209\n163/163 - 1s - 9ms/step - auc: 0.8275 - binary_accuracy: 0.7419 - loss: 0.5050 - precision: 0.8879 - recall: 0.7122 - val_auc: 0.7441 - val_binary_accuracy: 0.6723 - val_loss: 0.6163 - val_precision: 0.8314 - val_recall: 0.6527 - learning_rate: 5.0000e-04\nEpoch 19/30\n\nEpoch 19: val_loss did not improve from 0.57500\n163/163 - 1s - 8ms/step - auc: 0.8405 - binary_accuracy: 0.7583 - loss: 0.4889 - precision: 0.8856 - recall: 0.7420 - val_auc: 0.7468 - val_binary_accuracy: 0.6759 - val_loss: 0.6370 - val_precision: 0.8416 - val_recall: 0.6474 - learning_rate: 5.0000e-04\nEpoch 20/30\n\nEpoch 20: val_loss did not improve from 0.57500\n\nEpoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n[ThresholdOptimizer] Searching threshold at epoch 20 ...\n[ThresholdOptimizer] Best threshold so far: 0.320 with minority F1=0.8209\n163/163 - 1s - 9ms/step - auc: 0.8466 - binary_accuracy: 0.7613 - loss: 0.4785 - precision: 0.8901 - recall: 0.7423 - val_auc: 0.7490 - val_binary_accuracy: 0.6750 - val_loss: 0.6351 - val_precision: 0.8344 - val_recall: 0.6540 - learning_rate: 5.0000e-04\nEpoch 20: early stopping\nRestoring model weights from the end of the best epoch: 14.\n[train_and_evaluate_model] Final selected threshold for Transformer: 0.32\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n[train_and_evaluate_model] Test metrics for Transformer at threshold 0.320:\n  accuracy: 0.7243\n  precision: 0.7392\n  recall: 0.9214\n  f1: 0.8203\n  minority_precision: 0.7392\n  minority_recall: 0.9214\n  minority_f1: 0.8203\n[train_and_evaluate_model] Saved metrics to ./BASE_DIR/imbalance_analysis/results_Transformer_baseline.csv\n\n[train_and_evaluate_model] Starting training for 'Transformer' (aggressive=True)\n\n[Train-Transformer] Class distribution:\n  Class 0: 1650 samples\n  Class 1: 3558 samples\n  Imbalance ratio (maj/min): 2.156\n\n[Val-Transformer] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n\n[Test-Transformer] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n[train_and_evaluate_model] Target time frames for model input: 94\n[train_and_evaluate_model] Applying heavy spec_augment to minority samples for Transformer...\n[train_and_evaluate_model] Created 10674 augmented MFCC samples for minority.\n[train_and_evaluate_model] New training size: 15882\n[train_and_evaluate_model] Class weights used for training: {0: 4.8127272727272725, 1: 0.5579679595278246}\n[train_and_evaluate_model] Using dynamic focal loss with alpha_pos=0.5000\n\n[Model Architecture - Transformer]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_12\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_12\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_mfcc          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │         \u001b[38;5;34m80\u001b[0m │ input_mfcc[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │     \u001b[38;5;34m41,768\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │         \u001b[38;5;34m80\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ sequential_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │     \u001b[38;5;34m10,408\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_5 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ sequential_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │         \u001b[38;5;34m80\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │     \u001b[38;5;34m41,768\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_6 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │         \u001b[38;5;34m80\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ sequential_3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │     \u001b[38;5;34m10,408\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_7 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│                     │                   │            │ sequential_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │         \u001b[38;5;34m80\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_45          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_25 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m2,624\u001b[0m │ dropout_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_46          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ out (\u001b[38;5;33mDense\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_mfcc          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │ input_mfcc[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,768</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ sequential_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,408</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ sequential_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,768</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ multi_head_atten… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ sequential_3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,408</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│                     │                   │            │ sequential_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_45          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,624</span> │ dropout_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_46          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m107,441\u001b[0m (419.69 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">107,441</span> (419.69 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m107,441\u001b[0m (419.69 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">107,441</span> (419.69 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"[train_and_evaluate_model] Starting model.fit for Transformer...\nEpoch 1/30\n\nEpoch 1: val_loss improved from inf to 0.07812, saving model to ./BASE_DIR/models/robust_models/model_Transformer_robust.h5\n497/497 - 29s - 57ms/step - auc: 0.8565 - binary_accuracy: 0.8720 - loss: 0.0332 - precision: 0.9031 - recall: 0.9602 - val_auc: 0.5754 - val_binary_accuracy: 0.6831 - val_loss: 0.0781 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 2/30\n\nEpoch 2: val_loss improved from 0.07812 to 0.07778, saving model to ./BASE_DIR/models/robust_models/model_Transformer_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 2 ...\n[ThresholdOptimizer] Best threshold so far: 0.000 with minority F1=0.8117\n497/497 - 6s - 11ms/step - auc: 0.8665 - binary_accuracy: 0.8947 - loss: 0.0292 - precision: 0.8965 - recall: 0.9977 - val_auc: 0.5934 - val_binary_accuracy: 0.6831 - val_loss: 0.0778 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 3/30\n\nEpoch 3: val_loss did not improve from 0.07778\n497/497 - 3s - 7ms/step - auc: 0.8704 - binary_accuracy: 0.8949 - loss: 0.0288 - precision: 0.8973 - recall: 0.9968 - val_auc: 0.5968 - val_binary_accuracy: 0.6831 - val_loss: 0.0780 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 4/30\n\nEpoch 4: val_loss improved from 0.07778 to 0.07699, saving model to ./BASE_DIR/models/robust_models/model_Transformer_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 4 ...\n[ThresholdOptimizer] Best threshold so far: 0.000 with minority F1=0.8117\n497/497 - 4s - 7ms/step - auc: 0.8716 - binary_accuracy: 0.8951 - loss: 0.0287 - precision: 0.8971 - recall: 0.9974 - val_auc: 0.5641 - val_binary_accuracy: 0.6831 - val_loss: 0.0770 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 5/30\n\nEpoch 5: val_loss improved from 0.07699 to 0.07620, saving model to ./BASE_DIR/models/robust_models/model_Transformer_robust.h5\n497/497 - 3s - 7ms/step - auc: 0.8812 - binary_accuracy: 0.8960 - loss: 0.0282 - precision: 0.8962 - recall: 0.9997 - val_auc: 0.6311 - val_binary_accuracy: 0.6831 - val_loss: 0.0762 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 6/30\n\nEpoch 6: val_loss did not improve from 0.07620\n[ThresholdOptimizer] Searching threshold at epoch 6 ...\n[ThresholdOptimizer] Best threshold so far: 0.000 with minority F1=0.8117\n497/497 - 4s - 7ms/step - auc: 0.8814 - binary_accuracy: 0.8961 - loss: 0.0284 - precision: 0.8962 - recall: 0.9999 - val_auc: 0.5732 - val_binary_accuracy: 0.6831 - val_loss: 0.0775 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 7/30\n\nEpoch 7: val_loss improved from 0.07620 to 0.07499, saving model to ./BASE_DIR/models/robust_models/model_Transformer_robust.h5\n497/497 - 3s - 7ms/step - auc: 0.8830 - binary_accuracy: 0.8957 - loss: 0.0283 - precision: 0.8970 - recall: 0.9982 - val_auc: 0.6377 - val_binary_accuracy: 0.6831 - val_loss: 0.0750 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 8/30\n\nEpoch 8: val_loss improved from 0.07499 to 0.07483, saving model to ./BASE_DIR/models/robust_models/model_Transformer_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 8 ...\n[ThresholdOptimizer] Best threshold so far: 0.540 with minority F1=0.8121\n497/497 - 4s - 8ms/step - auc: 0.8940 - binary_accuracy: 0.8959 - loss: 0.0282 - precision: 0.8961 - recall: 0.9998 - val_auc: 0.6366 - val_binary_accuracy: 0.6831 - val_loss: 0.0748 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 9/30\n\nEpoch 9: val_loss did not improve from 0.07483\n497/497 - 3s - 7ms/step - auc: 0.8952 - binary_accuracy: 0.8961 - loss: 0.0277 - precision: 0.8961 - recall: 1.0000 - val_auc: 0.6294 - val_binary_accuracy: 0.6831 - val_loss: 0.0752 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 10/30\n\nEpoch 10: val_loss did not improve from 0.07483\n[ThresholdOptimizer] Searching threshold at epoch 10 ...\n[ThresholdOptimizer] Best threshold so far: 0.540 with minority F1=0.8121\n497/497 - 4s - 7ms/step - auc: 0.8862 - binary_accuracy: 0.8962 - loss: 0.0286 - precision: 0.8963 - recall: 0.9998 - val_auc: 0.5562 - val_binary_accuracy: 0.6831 - val_loss: 0.0771 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 11/30\n\nEpoch 11: val_loss did not improve from 0.07483\n\nEpoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n497/497 - 3s - 7ms/step - auc: 0.8817 - binary_accuracy: 0.8963 - loss: 0.0281 - precision: 0.8964 - recall: 0.9999 - val_auc: 0.5556 - val_binary_accuracy: 0.6831 - val_loss: 0.0773 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 12/30\n\nEpoch 12: val_loss improved from 0.07483 to 0.07423, saving model to ./BASE_DIR/models/robust_models/model_Transformer_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 12 ...\n[ThresholdOptimizer] Best threshold so far: 0.540 with minority F1=0.8121\n497/497 - 4s - 7ms/step - auc: 0.8962 - binary_accuracy: 0.8960 - loss: 0.0275 - precision: 0.8961 - recall: 0.9998 - val_auc: 0.6293 - val_binary_accuracy: 0.6831 - val_loss: 0.0742 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 5.0000e-04\nEpoch 13/30\n\nEpoch 13: val_loss improved from 0.07423 to 0.07382, saving model to ./BASE_DIR/models/robust_models/model_Transformer_robust.h5\n497/497 - 3s - 7ms/step - auc: 0.9045 - binary_accuracy: 0.8958 - loss: 0.0269 - precision: 0.8962 - recall: 0.9995 - val_auc: 0.6614 - val_binary_accuracy: 0.6831 - val_loss: 0.0738 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 5.0000e-04\nEpoch 14/30\n\nEpoch 14: val_loss did not improve from 0.07382\n[ThresholdOptimizer] Searching threshold at epoch 14 ...\n[ThresholdOptimizer] Best threshold so far: 0.540 with minority F1=0.8121\n497/497 - 4s - 7ms/step - auc: 0.9057 - binary_accuracy: 0.8952 - loss: 0.0268 - precision: 0.8985 - recall: 0.9955 - val_auc: 0.6337 - val_binary_accuracy: 0.6831 - val_loss: 0.0753 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 5.0000e-04\nEpoch 15/30\n\nEpoch 15: val_loss did not improve from 0.07382\n497/497 - 3s - 7ms/step - auc: 0.9065 - binary_accuracy: 0.8959 - loss: 0.0268 - precision: 0.8966 - recall: 0.9991 - val_auc: 0.6715 - val_binary_accuracy: 0.6831 - val_loss: 0.0742 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 5.0000e-04\nEpoch 16/30\n\nEpoch 16: val_loss did not improve from 0.07382\n\nEpoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n[ThresholdOptimizer] Searching threshold at epoch 16 ...\n[ThresholdOptimizer] Best threshold so far: 0.540 with minority F1=0.8121\n497/497 - 4s - 7ms/step - auc: 0.9132 - binary_accuracy: 0.8952 - loss: 0.0263 - precision: 0.9022 - recall: 0.9903 - val_auc: 0.6904 - val_binary_accuracy: 0.6831 - val_loss: 0.0748 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 5.0000e-04\nEpoch 17/30\n\nEpoch 17: val_loss improved from 0.07382 to 0.07342, saving model to ./BASE_DIR/models/robust_models/model_Transformer_robust.h5\n497/497 - 3s - 7ms/step - auc: 0.9207 - binary_accuracy: 0.8961 - loss: 0.0258 - precision: 0.9105 - recall: 0.9805 - val_auc: 0.7082 - val_binary_accuracy: 0.7028 - val_loss: 0.0734 - val_precision: 0.7446 - val_recall: 0.8598 - learning_rate: 2.5000e-04\nEpoch 18/30\n\nEpoch 18: val_loss improved from 0.07342 to 0.07334, saving model to ./BASE_DIR/models/robust_models/model_Transformer_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 18 ...\n[ThresholdOptimizer] Best threshold so far: 0.540 with minority F1=0.8121\n497/497 - 4s - 7ms/step - auc: 0.9230 - binary_accuracy: 0.8937 - loss: 0.0254 - precision: 0.9146 - recall: 0.9720 - val_auc: 0.7102 - val_binary_accuracy: 0.7019 - val_loss: 0.0733 - val_precision: 0.7688 - val_recall: 0.8060 - learning_rate: 2.5000e-04\nEpoch 19/30\n\nEpoch 19: val_loss did not improve from 0.07334\n497/497 - 3s - 7ms/step - auc: 0.9265 - binary_accuracy: 0.8971 - loss: 0.0250 - precision: 0.9220 - recall: 0.9670 - val_auc: 0.7157 - val_binary_accuracy: 0.6535 - val_loss: 0.0741 - val_precision: 0.8082 - val_recall: 0.6461 - learning_rate: 2.5000e-04\nEpoch 20/30\n\nEpoch 20: val_loss improved from 0.07334 to 0.07294, saving model to ./BASE_DIR/models/robust_models/model_Transformer_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 20 ...\n[ThresholdOptimizer] Best threshold so far: 0.540 with minority F1=0.8121\n497/497 - 4s - 7ms/step - auc: 0.9281 - binary_accuracy: 0.8976 - loss: 0.0248 - precision: 0.9244 - recall: 0.9646 - val_auc: 0.7225 - val_binary_accuracy: 0.6983 - val_loss: 0.0729 - val_precision: 0.7825 - val_recall: 0.7733 - learning_rate: 2.5000e-04\nEpoch 21/30\n\nEpoch 21: val_loss did not improve from 0.07294\n497/497 - 3s - 7ms/step - auc: 0.9299 - binary_accuracy: 0.8983 - loss: 0.0245 - precision: 0.9254 - recall: 0.9643 - val_auc: 0.7352 - val_binary_accuracy: 0.7055 - val_loss: 0.0781 - val_precision: 0.7789 - val_recall: 0.7942 - learning_rate: 2.5000e-04\nEpoch 22/30\n\nEpoch 22: val_loss did not improve from 0.07294\n[ThresholdOptimizer] Searching threshold at epoch 22 ...\n[ThresholdOptimizer] Best threshold so far: 0.480 with minority F1=0.8134\n497/497 - 4s - 7ms/step - auc: 0.9318 - binary_accuracy: 0.9009 - loss: 0.0241 - precision: 0.9293 - recall: 0.9627 - val_auc: 0.7347 - val_binary_accuracy: 0.7055 - val_loss: 0.0756 - val_precision: 0.7811 - val_recall: 0.7903 - learning_rate: 2.5000e-04\nEpoch 23/30\n\nEpoch 23: val_loss did not improve from 0.07294\n\nEpoch 23: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n497/497 - 3s - 7ms/step - auc: 0.9327 - binary_accuracy: 0.9012 - loss: 0.0242 - precision: 0.9326 - recall: 0.9591 - val_auc: 0.7210 - val_binary_accuracy: 0.6732 - val_loss: 0.0796 - val_precision: 0.7926 - val_recall: 0.7064 - learning_rate: 2.5000e-04\nEpoch 24/30\n\nEpoch 24: val_loss did not improve from 0.07294\n[ThresholdOptimizer] Searching threshold at epoch 24 ...\n[ThresholdOptimizer] Best threshold so far: 0.470 with minority F1=0.8154\n497/497 - 4s - 7ms/step - auc: 0.9358 - binary_accuracy: 0.9026 - loss: 0.0239 - precision: 0.9333 - recall: 0.9599 - val_auc: 0.7324 - val_binary_accuracy: 0.6768 - val_loss: 0.0760 - val_precision: 0.7847 - val_recall: 0.7261 - learning_rate: 1.2500e-04\nEpoch 25/30\n\nEpoch 25: val_loss did not improve from 0.07294\n497/497 - 3s - 7ms/step - auc: 0.9378 - binary_accuracy: 0.9027 - loss: 0.0234 - precision: 0.9394 - recall: 0.9529 - val_auc: 0.7372 - val_binary_accuracy: 0.6929 - val_loss: 0.0806 - val_precision: 0.7749 - val_recall: 0.7759 - learning_rate: 1.2500e-04\nEpoch 26/30\n\nEpoch 26: val_loss did not improve from 0.07294\n\nEpoch 26: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n[ThresholdOptimizer] Searching threshold at epoch 26 ...\n[ThresholdOptimizer] Best threshold so far: 0.470 with minority F1=0.8172\n497/497 - 4s - 7ms/step - auc: 0.9377 - binary_accuracy: 0.9054 - loss: 0.0235 - precision: 0.9402 - recall: 0.9551 - val_auc: 0.7388 - val_binary_accuracy: 0.6920 - val_loss: 0.0788 - val_precision: 0.7790 - val_recall: 0.7667 - learning_rate: 1.2500e-04\nEpoch 26: early stopping\nRestoring model weights from the end of the best epoch: 20.\n[train_and_evaluate_model] Final selected threshold for Transformer: 0.47000000000000003\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n[train_and_evaluate_model] Test metrics for Transformer at threshold 0.470:\n  accuracy: 0.6831\n  precision: 0.6831\n  recall: 1.0000\n  f1: 0.8117\n  minority_precision: 0.6831\n  minority_recall: 1.0000\n  minority_f1: 0.8117\n[train_and_evaluate_model] Saved metrics to ./BASE_DIR/imbalance_analysis/results_Transformer_robust.csv\n\n============================================================\nTRAINING MODEL: Hybrid_CNN_LSTM\n============================================================\n\n[train_and_evaluate_model] Starting training for 'Hybrid_CNN_LSTM' (aggressive=False)\n\n[Train-Hybrid_CNN_LSTM] Class distribution:\n  Class 0: 1650 samples\n  Class 1: 3558 samples\n  Imbalance ratio (maj/min): 2.156\n\n[Val-Hybrid_CNN_LSTM] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n\n[Test-Hybrid_CNN_LSTM] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n[train_and_evaluate_model] Target time frames for model input: 94\n[train_and_evaluate_model] Class weights used for training: {0: 1.5781818181818181, 1: 0.7318718381112985}\n[train_and_evaluate_model] Using BinaryCrossentropy loss (baseline).\n\n[Model Architecture - Hybrid_CNN_LSTM]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_13\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_13\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_mfcc          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast_8 (\u001b[38;5;33mCast\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ input_mfcc[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m12,864\u001b[0m │ cast_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m256\u001b[0m │ conv1d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling1d_17    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast_9 (\u001b[38;5;33mCast\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ input_mfcc[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_47          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_17… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional_12    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m173,056\u001b[0m │ cast_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_12 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m24,704\u001b[0m │ dropout_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ bidirectional_12… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ conv1d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_49          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling1d_18    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional_13    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m164,352\u001b[0m │ dropout_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_48          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_18… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ bidirectional_13… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_50          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_26 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_51          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_27 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_52          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_28 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_53          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ out (\u001b[38;5;33mDense\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_mfcc          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_mfcc[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,864</span> │ cast_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling1d_17    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_mfcc[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_47          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_17… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional_12    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">173,056</span> │ cast_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ dropout_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ bidirectional_12… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_49          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling1d_18    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional_13    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │ dropout_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_48          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_18… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ bidirectional_13… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_50          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_51          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_52          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_53          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m485,313\u001b[0m (1.85 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">485,313</span> (1.85 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m483,649\u001b[0m (1.84 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">483,649</span> (1.84 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,664\u001b[0m (6.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span> (6.50 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"[train_and_evaluate_model] Starting model.fit for Hybrid_CNN_LSTM...\nEpoch 1/30\n\nEpoch 1: val_loss improved from inf to 0.63457, saving model to ./BASE_DIR/models/robust_models/model_Hybrid_CNN_LSTM_baseline.h5\n163/163 - 19s - 116ms/step - auc: 0.6171 - binary_accuracy: 0.5845 - loss: 0.7078 - precision: 0.7559 - recall: 0.5787 - val_auc: 0.5841 - val_binary_accuracy: 0.6061 - val_loss: 0.6346 - val_precision: 0.6761 - val_recall: 0.8126 - learning_rate: 0.0010\nEpoch 2/30\n\nEpoch 2: val_loss did not improve from 0.63457\n[ThresholdOptimizer] Searching threshold at epoch 2 ...\n[ThresholdOptimizer] Best threshold so far: 0.300 with minority F1=0.8183\n163/163 - 8s - 51ms/step - auc: 0.6772 - binary_accuracy: 0.6104 - loss: 0.6500 - precision: 0.7942 - recall: 0.5801 - val_auc: 0.7412 - val_binary_accuracy: 0.6115 - val_loss: 0.6471 - val_precision: 0.8584 - val_recall: 0.5164 - learning_rate: 0.0010\nEpoch 3/30\n\nEpoch 3: val_loss improved from 0.63457 to 0.55457, saving model to ./BASE_DIR/models/robust_models/model_Hybrid_CNN_LSTM_baseline.h5\n163/163 - 6s - 39ms/step - auc: 0.7087 - binary_accuracy: 0.6440 - loss: 0.6221 - precision: 0.8174 - recall: 0.6166 - val_auc: 0.7289 - val_binary_accuracy: 0.7046 - val_loss: 0.5546 - val_precision: 0.7624 - val_recall: 0.8244 - learning_rate: 0.0010\nEpoch 4/30\n\nEpoch 4: val_loss improved from 0.55457 to 0.52530, saving model to ./BASE_DIR/models/robust_models/model_Hybrid_CNN_LSTM_baseline.h5\n[ThresholdOptimizer] Searching threshold at epoch 4 ...\n[ThresholdOptimizer] Best threshold so far: 0.410 with minority F1=0.8212\n163/163 - 7s - 41ms/step - auc: 0.7376 - binary_accuracy: 0.6567 - loss: 0.6018 - precision: 0.8297 - recall: 0.6259 - val_auc: 0.7720 - val_binary_accuracy: 0.7144 - val_loss: 0.5253 - val_precision: 0.7472 - val_recall: 0.8794 - learning_rate: 0.0010\nEpoch 5/30\n\nEpoch 5: val_loss did not improve from 0.52530\n163/163 - 6s - 38ms/step - auc: 0.7574 - binary_accuracy: 0.6751 - loss: 0.5799 - precision: 0.8577 - recall: 0.6287 - val_auc: 0.7758 - val_binary_accuracy: 0.5944 - val_loss: 0.6491 - val_precision: 0.9235 - val_recall: 0.4430 - learning_rate: 0.0010\nEpoch 6/30\n\nEpoch 6: val_loss improved from 0.52530 to 0.50498, saving model to ./BASE_DIR/models/robust_models/model_Hybrid_CNN_LSTM_baseline.h5\n[ThresholdOptimizer] Searching threshold at epoch 6 ...\n[ThresholdOptimizer] Best threshold so far: 0.370 with minority F1=0.8282\n163/163 - 7s - 41ms/step - auc: 0.7691 - binary_accuracy: 0.6832 - loss: 0.5717 - precision: 0.8655 - recall: 0.6349 - val_auc: 0.7807 - val_binary_accuracy: 0.7046 - val_loss: 0.5050 - val_precision: 0.7808 - val_recall: 0.7890 - learning_rate: 0.0010\nEpoch 7/30\n\nEpoch 7: val_loss did not improve from 0.50498\n163/163 - 6s - 38ms/step - auc: 0.7829 - binary_accuracy: 0.6911 - loss: 0.5562 - precision: 0.8695 - recall: 0.6445 - val_auc: 0.7934 - val_binary_accuracy: 0.7413 - val_loss: 0.5109 - val_precision: 0.7712 - val_recall: 0.8834 - learning_rate: 0.0010\nEpoch 8/30\n\nEpoch 8: val_loss did not improve from 0.50498\n[ThresholdOptimizer] Searching threshold at epoch 8 ...\n[ThresholdOptimizer] Best threshold so far: 0.370 with minority F1=0.8282\n163/163 - 7s - 40ms/step - auc: 0.7954 - binary_accuracy: 0.7008 - loss: 0.5441 - precision: 0.8843 - recall: 0.6467 - val_auc: 0.7834 - val_binary_accuracy: 0.7207 - val_loss: 0.5114 - val_precision: 0.8003 - val_recall: 0.7877 - learning_rate: 0.0010\nEpoch 9/30\n\nEpoch 9: val_loss did not improve from 0.50498\n\nEpoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n163/163 - 6s - 38ms/step - auc: 0.8097 - binary_accuracy: 0.7079 - loss: 0.5259 - precision: 0.8877 - recall: 0.6554 - val_auc: 0.7449 - val_binary_accuracy: 0.6974 - val_loss: 0.6000 - val_precision: 0.7292 - val_recall: 0.8860 - learning_rate: 0.0010\nEpoch 10/30\n\nEpoch 10: val_loss did not improve from 0.50498\n[ThresholdOptimizer] Searching threshold at epoch 10 ...\n[ThresholdOptimizer] Best threshold so far: 0.360 with minority F1=0.8348\n163/163 - 7s - 40ms/step - auc: 0.8414 - binary_accuracy: 0.7446 - loss: 0.4865 - precision: 0.9057 - recall: 0.6990 - val_auc: 0.8064 - val_binary_accuracy: 0.7502 - val_loss: 0.5233 - val_precision: 0.8071 - val_recall: 0.8336 - learning_rate: 5.0000e-04\nEpoch 11/30\n\nEpoch 11: val_loss did not improve from 0.50498\n163/163 - 6s - 38ms/step - auc: 0.8471 - binary_accuracy: 0.7477 - loss: 0.4793 - precision: 0.9027 - recall: 0.7069 - val_auc: 0.8153 - val_binary_accuracy: 0.7359 - val_loss: 0.5084 - val_precision: 0.8634 - val_recall: 0.7287 - learning_rate: 5.0000e-04\nEpoch 12/30\n\nEpoch 12: val_loss did not improve from 0.50498\n\nEpoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n[ThresholdOptimizer] Searching threshold at epoch 12 ...\n[ThresholdOptimizer] Best threshold so far: 0.360 with minority F1=0.8395\n163/163 - 7s - 40ms/step - auc: 0.8634 - binary_accuracy: 0.7621 - loss: 0.4540 - precision: 0.9146 - recall: 0.7189 - val_auc: 0.8119 - val_binary_accuracy: 0.7681 - val_loss: 0.5713 - val_precision: 0.8127 - val_recall: 0.8585 - learning_rate: 5.0000e-04\nEpoch 12: early stopping\nRestoring model weights from the end of the best epoch: 6.\n[train_and_evaluate_model] Final selected threshold for Hybrid_CNN_LSTM: 0.36\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n[train_and_evaluate_model] Test metrics for Hybrid_CNN_LSTM at threshold 0.360:\n  accuracy: 0.7296\n  precision: 0.7284\n  recall: 0.9633\n  f1: 0.8296\n  minority_precision: 0.7284\n  minority_recall: 0.9633\n  minority_f1: 0.8296\n[train_and_evaluate_model] Saved metrics to ./BASE_DIR/imbalance_analysis/results_Hybrid_CNN_LSTM_baseline.csv\n\n[train_and_evaluate_model] Starting training for 'Hybrid_CNN_LSTM' (aggressive=True)\n\n[Train-Hybrid_CNN_LSTM] Class distribution:\n  Class 0: 1650 samples\n  Class 1: 3558 samples\n  Imbalance ratio (maj/min): 2.156\n\n[Val-Hybrid_CNN_LSTM] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n\n[Test-Hybrid_CNN_LSTM] Class distribution:\n  Class 0: 354 samples\n  Class 1: 763 samples\n  Imbalance ratio (maj/min): 2.155\n[train_and_evaluate_model] Target time frames for model input: 94\n[train_and_evaluate_model] Applying heavy spec_augment to minority samples for Hybrid_CNN_LSTM...\n[train_and_evaluate_model] Created 10674 augmented MFCC samples for minority.\n[train_and_evaluate_model] New training size: 15882\n[train_and_evaluate_model] Class weights used for training: {0: 4.8127272727272725, 1: 0.5579679595278246}\n[train_and_evaluate_model] Using dynamic focal loss with alpha_pos=0.5000\n\n[Model Architecture - Hybrid_CNN_LSTM]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_14\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_14\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_mfcc          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast_10 (\u001b[38;5;33mCast\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ input_mfcc[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_13 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m12,864\u001b[0m │ cast_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m256\u001b[0m │ conv1d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling1d_19    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast_11 (\u001b[38;5;33mCast\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m40\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ input_mfcc[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_54          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_19… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional_14    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m173,056\u001b[0m │ cast_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_14 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m24,704\u001b[0m │ dropout_54[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │      \u001b[38;5;34m1,024\u001b[0m │ bidirectional_14… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │        \u001b[38;5;34m512\u001b[0m │ conv1d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_56          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling1d_20    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional_15    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m164,352\u001b[0m │ dropout_56[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_55          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_20… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ bidirectional_15… │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_55[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_57          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_29 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_58          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_30 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_59          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_31 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_60          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ out (\u001b[38;5;33mDense\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_mfcc          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_mfcc[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,864</span> │ cast_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling1d_19    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ cast_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_mfcc[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_54          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_19… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional_14    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">173,056</span> │ cast_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ dropout_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ bidirectional_14… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_56          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ max_pooling1d_20    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bidirectional_15    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │ dropout_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_55          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_20… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ bidirectional_15… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_57          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_58          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_59          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_60          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m485,313\u001b[0m (1.85 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">485,313</span> (1.85 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m483,649\u001b[0m (1.84 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">483,649</span> (1.84 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,664\u001b[0m (6.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span> (6.50 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"[train_and_evaluate_model] Starting model.fit for Hybrid_CNN_LSTM...\nEpoch 1/30\n\nEpoch 1: val_loss improved from inf to 0.07860, saving model to ./BASE_DIR/models/robust_models/model_Hybrid_CNN_LSTM_robust.h5\n497/497 - 30s - 60ms/step - auc: 0.8341 - binary_accuracy: 0.8684 - loss: 0.0420 - precision: 0.9119 - recall: 0.9444 - val_auc: 0.6394 - val_binary_accuracy: 0.6831 - val_loss: 0.0786 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 2/30\n\nEpoch 2: val_loss did not improve from 0.07860\n[ThresholdOptimizer] Searching threshold at epoch 2 ...\n[ThresholdOptimizer] Best threshold so far: 0.000 with minority F1=0.8117\n497/497 - 19s - 39ms/step - auc: 0.8931 - binary_accuracy: 0.8883 - loss: 0.0293 - precision: 0.9055 - recall: 0.9774 - val_auc: 0.6561 - val_binary_accuracy: 0.6831 - val_loss: 0.0803 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 3/30\n\nEpoch 3: val_loss improved from 0.07860 to 0.07714, saving model to ./BASE_DIR/models/robust_models/model_Hybrid_CNN_LSTM_robust.h5\n497/497 - 17s - 35ms/step - auc: 0.9006 - binary_accuracy: 0.8922 - loss: 0.0282 - precision: 0.9038 - recall: 0.9845 - val_auc: 0.6928 - val_binary_accuracy: 0.6831 - val_loss: 0.0771 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 4/30\n\nEpoch 4: val_loss improved from 0.07714 to 0.07642, saving model to ./BASE_DIR/models/robust_models/model_Hybrid_CNN_LSTM_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 4 ...\n[ThresholdOptimizer] Best threshold so far: 0.510 with minority F1=0.8140\n497/497 - 18s - 36ms/step - auc: 0.9083 - binary_accuracy: 0.8941 - loss: 0.0273 - precision: 0.9077 - recall: 0.9817 - val_auc: 0.7156 - val_binary_accuracy: 0.6831 - val_loss: 0.0764 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 5/30\n\nEpoch 5: val_loss improved from 0.07642 to 0.07414, saving model to ./BASE_DIR/models/robust_models/model_Hybrid_CNN_LSTM_robust.h5\n497/497 - 18s - 35ms/step - auc: 0.9126 - binary_accuracy: 0.8952 - loss: 0.0271 - precision: 0.9078 - recall: 0.9828 - val_auc: 0.6575 - val_binary_accuracy: 0.6831 - val_loss: 0.0741 - val_precision: 0.6844 - val_recall: 0.9948 - learning_rate: 0.0010\nEpoch 6/30\n\nEpoch 6: val_loss improved from 0.07414 to 0.07285, saving model to ./BASE_DIR/models/robust_models/model_Hybrid_CNN_LSTM_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 6 ...\n[ThresholdOptimizer] Best threshold so far: 0.510 with minority F1=0.8140\n497/497 - 18s - 36ms/step - auc: 0.9179 - binary_accuracy: 0.8954 - loss: 0.0263 - precision: 0.9119 - recall: 0.9777 - val_auc: 0.7384 - val_binary_accuracy: 0.6831 - val_loss: 0.0728 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 7/30\n\nEpoch 7: val_loss improved from 0.07285 to 0.06960, saving model to ./BASE_DIR/models/robust_models/model_Hybrid_CNN_LSTM_robust.h5\n497/497 - 18s - 36ms/step - auc: 0.9212 - binary_accuracy: 0.8955 - loss: 0.0261 - precision: 0.9165 - recall: 0.9719 - val_auc: 0.7598 - val_binary_accuracy: 0.7019 - val_loss: 0.0696 - val_precision: 0.7059 - val_recall: 0.9659 - learning_rate: 0.0010\nEpoch 8/30\n\nEpoch 8: val_loss did not improve from 0.06960\n[ThresholdOptimizer] Searching threshold at epoch 8 ...\n[ThresholdOptimizer] Best threshold so far: 0.510 with minority F1=0.8140\n497/497 - 18s - 36ms/step - auc: 0.9272 - binary_accuracy: 0.9001 - loss: 0.0252 - precision: 0.9234 - recall: 0.9689 - val_auc: 0.6498 - val_binary_accuracy: 0.6831 - val_loss: 0.0782 - val_precision: 0.6831 - val_recall: 1.0000 - learning_rate: 0.0010\nEpoch 9/30\n\nEpoch 9: val_loss improved from 0.06960 to 0.06785, saving model to ./BASE_DIR/models/robust_models/model_Hybrid_CNN_LSTM_robust.h5\n497/497 - 18s - 35ms/step - auc: 0.9305 - binary_accuracy: 0.9008 - loss: 0.0247 - precision: 0.9262 - recall: 0.9663 - val_auc: 0.7656 - val_binary_accuracy: 0.7028 - val_loss: 0.0679 - val_precision: 0.7047 - val_recall: 0.9725 - learning_rate: 0.0010\nEpoch 10/30\n\nEpoch 10: val_loss did not improve from 0.06785\n[ThresholdOptimizer] Searching threshold at epoch 10 ...\n[ThresholdOptimizer] Best threshold so far: 0.500 with minority F1=0.8141\n497/497 - 18s - 36ms/step - auc: 0.9294 - binary_accuracy: 0.8997 - loss: 0.0247 - precision: 0.9238 - recall: 0.9680 - val_auc: 0.7443 - val_binary_accuracy: 0.7001 - val_loss: 0.0711 - val_precision: 0.7046 - val_recall: 0.9659 - learning_rate: 0.0010\nEpoch 11/30\n\nEpoch 11: val_loss did not improve from 0.06785\n497/497 - 18s - 35ms/step - auc: 0.9371 - binary_accuracy: 0.9019 - loss: 0.0236 - precision: 0.9311 - recall: 0.9617 - val_auc: 0.7878 - val_binary_accuracy: 0.6992 - val_loss: 0.0733 - val_precision: 0.6990 - val_recall: 0.9830 - learning_rate: 0.0010\nEpoch 12/30\n\nEpoch 12: val_loss improved from 0.06785 to 0.06720, saving model to ./BASE_DIR/models/robust_models/model_Hybrid_CNN_LSTM_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 12 ...\n[ThresholdOptimizer] Best threshold so far: 0.520 with minority F1=0.8218\n497/497 - 18s - 36ms/step - auc: 0.9369 - binary_accuracy: 0.9040 - loss: 0.0238 - precision: 0.9343 - recall: 0.9605 - val_auc: 0.7844 - val_binary_accuracy: 0.7099 - val_loss: 0.0672 - val_precision: 0.7117 - val_recall: 0.9672 - learning_rate: 0.0010\nEpoch 13/30\n\nEpoch 13: val_loss did not improve from 0.06720\n497/497 - 18s - 35ms/step - auc: 0.9407 - binary_accuracy: 0.9050 - loss: 0.0231 - precision: 0.9345 - recall: 0.9614 - val_auc: 0.7095 - val_binary_accuracy: 0.6849 - val_loss: 0.1255 - val_precision: 0.6860 - val_recall: 0.9934 - learning_rate: 0.0010\nEpoch 14/30\n\nEpoch 14: val_loss improved from 0.06720 to 0.06681, saving model to ./BASE_DIR/models/robust_models/model_Hybrid_CNN_LSTM_robust.h5\n[ThresholdOptimizer] Searching threshold at epoch 14 ...\n[ThresholdOptimizer] Best threshold so far: 0.460 with minority F1=0.8265\n497/497 - 18s - 36ms/step - auc: 0.9408 - binary_accuracy: 0.9086 - loss: 0.0234 - precision: 0.9373 - recall: 0.9624 - val_auc: 0.7754 - val_binary_accuracy: 0.7144 - val_loss: 0.0668 - val_precision: 0.8236 - val_recall: 0.7405 - learning_rate: 0.0010\nEpoch 15/30\n\nEpoch 15: val_loss did not improve from 0.06681\n497/497 - 18s - 35ms/step - auc: 0.9437 - binary_accuracy: 0.9074 - loss: 0.0227 - precision: 0.9383 - recall: 0.9599 - val_auc: 0.8252 - val_binary_accuracy: 0.7171 - val_loss: 0.0674 - val_precision: 0.9086 - val_recall: 0.6514 - learning_rate: 0.0010\nEpoch 16/30\n\nEpoch 16: val_loss did not improve from 0.06681\n[ThresholdOptimizer] Searching threshold at epoch 16 ...\n[ThresholdOptimizer] Best threshold so far: 0.460 with minority F1=0.8265\n497/497 - 18s - 36ms/step - auc: 0.9456 - binary_accuracy: 0.9083 - loss: 0.0225 - precision: 0.9432 - recall: 0.9551 - val_auc: 0.7817 - val_binary_accuracy: 0.7001 - val_loss: 0.0723 - val_precision: 0.8808 - val_recall: 0.6488 - learning_rate: 0.0010\nEpoch 17/30\n\nEpoch 17: val_loss improved from 0.06681 to 0.06396, saving model to ./BASE_DIR/models/robust_models/model_Hybrid_CNN_LSTM_robust.h5\n497/497 - 18s - 36ms/step - auc: 0.9484 - binary_accuracy: 0.9128 - loss: 0.0221 - precision: 0.9480 - recall: 0.9551 - val_auc: 0.7931 - val_binary_accuracy: 0.7314 - val_loss: 0.0640 - val_precision: 0.7753 - val_recall: 0.8545 - learning_rate: 0.0010\nEpoch 18/30\n\nEpoch 18: val_loss did not improve from 0.06396\n[ThresholdOptimizer] Searching threshold at epoch 18 ...\n[ThresholdOptimizer] Best threshold so far: 0.440 with minority F1=0.8368\n497/497 - 18s - 36ms/step - auc: 0.9506 - binary_accuracy: 0.9134 - loss: 0.0216 - precision: 0.9460 - recall: 0.9581 - val_auc: 0.8188 - val_binary_accuracy: 0.7395 - val_loss: 0.0653 - val_precision: 0.8782 - val_recall: 0.7182 - learning_rate: 0.0010\nEpoch 19/30\n\nEpoch 19: val_loss improved from 0.06396 to 0.06333, saving model to ./BASE_DIR/models/robust_models/model_Hybrid_CNN_LSTM_robust.h5\n497/497 - 18s - 36ms/step - auc: 0.9522 - binary_accuracy: 0.9172 - loss: 0.0216 - precision: 0.9511 - recall: 0.9568 - val_auc: 0.8122 - val_binary_accuracy: 0.7538 - val_loss: 0.0633 - val_precision: 0.7905 - val_recall: 0.8702 - learning_rate: 0.0010\nEpoch 20/30\n\nEpoch 20: val_loss did not improve from 0.06333\n[ThresholdOptimizer] Searching threshold at epoch 20 ...\n[ThresholdOptimizer] Best threshold so far: 0.490 with minority F1=0.8489\n497/497 - 18s - 36ms/step - auc: 0.9546 - binary_accuracy: 0.9205 - loss: 0.0213 - precision: 0.9553 - recall: 0.9559 - val_auc: 0.8241 - val_binary_accuracy: 0.7771 - val_loss: 0.0643 - val_precision: 0.8082 - val_recall: 0.8834 - learning_rate: 0.0010\nEpoch 21/30\n\nEpoch 21: val_loss improved from 0.06333 to 0.06130, saving model to ./BASE_DIR/models/robust_models/model_Hybrid_CNN_LSTM_robust.h5\n497/497 - 18s - 36ms/step - auc: 0.9566 - binary_accuracy: 0.9208 - loss: 0.0206 - precision: 0.9567 - recall: 0.9548 - val_auc: 0.8253 - val_binary_accuracy: 0.7332 - val_loss: 0.0613 - val_precision: 0.8627 - val_recall: 0.7248 - learning_rate: 0.0010\nEpoch 22/30\n\nEpoch 22: val_loss did not improve from 0.06130\n[ThresholdOptimizer] Searching threshold at epoch 22 ...\n[ThresholdOptimizer] Best threshold so far: 0.490 with minority F1=0.8489\n497/497 - 18s - 36ms/step - auc: 0.9577 - binary_accuracy: 0.9228 - loss: 0.0203 - precision: 0.9576 - recall: 0.9562 - val_auc: 0.8309 - val_binary_accuracy: 0.7529 - val_loss: 0.0629 - val_precision: 0.8740 - val_recall: 0.7457 - learning_rate: 0.0010\nEpoch 23/30\n\nEpoch 23: val_loss did not improve from 0.06130\n497/497 - 17s - 35ms/step - auc: 0.9607 - binary_accuracy: 0.9251 - loss: 0.0197 - precision: 0.9581 - recall: 0.9583 - val_auc: 0.8111 - val_binary_accuracy: 0.7198 - val_loss: 0.0956 - val_precision: 0.7210 - val_recall: 0.9620 - learning_rate: 0.0010\nEpoch 24/30\n\nEpoch 24: val_loss did not improve from 0.06130\n\nEpoch 24: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n[ThresholdOptimizer] Searching threshold at epoch 24 ...\n[ThresholdOptimizer] Best threshold so far: 0.490 with minority F1=0.8489\n497/497 - 18s - 36ms/step - auc: 0.9617 - binary_accuracy: 0.9291 - loss: 0.0197 - precision: 0.9606 - recall: 0.9602 - val_auc: 0.8308 - val_binary_accuracy: 0.7493 - val_loss: 0.0710 - val_precision: 0.7524 - val_recall: 0.9436 - learning_rate: 0.0010\nEpoch 25/30\n\nEpoch 25: val_loss did not improve from 0.06130\n497/497 - 17s - 35ms/step - auc: 0.9690 - binary_accuracy: 0.9341 - loss: 0.0178 - precision: 0.9655 - recall: 0.9608 - val_auc: 0.8416 - val_binary_accuracy: 0.7744 - val_loss: 0.0828 - val_precision: 0.7842 - val_recall: 0.9240 - learning_rate: 5.0000e-04\nEpoch 26/30\n\nEpoch 26: val_loss did not improve from 0.06130\n[ThresholdOptimizer] Searching threshold at epoch 26 ...\n[ThresholdOptimizer] Best threshold so far: 0.490 with minority F1=0.8489\n497/497 - 18s - 36ms/step - auc: 0.9709 - binary_accuracy: 0.9374 - loss: 0.0172 - precision: 0.9690 - recall: 0.9609 - val_auc: 0.8102 - val_binary_accuracy: 0.7305 - val_loss: 0.0850 - val_precision: 0.7343 - val_recall: 0.9489 - learning_rate: 5.0000e-04\nEpoch 27/30\n\nEpoch 27: val_loss did not improve from 0.06130\n\nEpoch 27: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n497/497 - 17s - 35ms/step - auc: 0.9720 - binary_accuracy: 0.9408 - loss: 0.0169 - precision: 0.9687 - recall: 0.9651 - val_auc: 0.8412 - val_binary_accuracy: 0.7780 - val_loss: 0.0622 - val_precision: 0.8099 - val_recall: 0.8820 - learning_rate: 5.0000e-04\nEpoch 27: early stopping\nRestoring model weights from the end of the best epoch: 21.\n[train_and_evaluate_model] Final selected threshold for Hybrid_CNN_LSTM: 0.49\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n[train_and_evaluate_model] Test metrics for Hybrid_CNN_LSTM at threshold 0.490:\n  accuracy: 0.7180\n  precision: 0.8625\n  recall: 0.6986\n  f1: 0.7719\n  minority_precision: 0.8625\n  minority_recall: 0.6986\n  minority_f1: 0.7719\n[train_and_evaluate_model] Saved metrics to ./BASE_DIR/imbalance_analysis/results_Hybrid_CNN_LSTM_robust.csv\n\n[compare_and_plot_results] Comparison table:\n                      model     architecture  strategy  threshold  accuracy  \\\n0       CNN_BiLSTM_baseline       CNN_BiLSTM  baseline       0.25  0.744852   \n1         CNN_BiLSTM_robust       CNN_BiLSTM    robust       0.50  0.744852   \n2         CNN_Only_baseline         CNN_Only  baseline       0.27  0.726052   \n3           CNN_Only_robust         CNN_Only    robust       0.45  0.764548   \n4             LSTM_baseline             LSTM  baseline       0.24  0.683080   \n5               LSTM_robust             LSTM    robust       0.41  0.742167   \n6      Transformer_baseline      Transformer  baseline       0.32  0.724261   \n7        Transformer_robust      Transformer    robust       0.47  0.683080   \n8  Hybrid_CNN_LSTM_baseline  Hybrid_CNN_LSTM  baseline       0.36  0.729633   \n9    Hybrid_CNN_LSTM_robust  Hybrid_CNN_LSTM    robust       0.49  0.717995   \n\n   precision    recall        f1  minority_precision  minority_recall  \\\n0   0.763797  0.906946  0.829239            0.763797         0.906946   \n1   0.804847  0.826999  0.815772            0.804847         0.826999   \n2   0.724680  0.965924  0.828090            0.724680         0.965924   \n3   0.765393  0.944954  0.845748            0.765393         0.944954   \n4   0.683080  1.000000  0.811702            0.683080         1.000000   \n5   0.734452  0.975098  0.837838            0.734452         0.975098   \n6   0.739222  0.921363  0.820303            0.739222         0.921363   \n7   0.683080  1.000000  0.811702            0.683080         1.000000   \n8   0.728444  0.963303  0.829571            0.728444         0.963303   \n9   0.862460  0.698558  0.771904            0.862460         0.698558   \n\n   minority_f1  \n0     0.829239  \n1     0.815772  \n2     0.828090  \n3     0.845748  \n4     0.811702  \n5     0.837838  \n6     0.820303  \n7     0.811702  \n8     0.829571  \n9     0.771904  \n[compare_and_plot_results] Saved comparison plot to ./BASE_DIR/visualizations/model_comparisons/all_models_comparison.png\n[main] Saved comparison CSV to ./BASE_DIR/imbalance_analysis/all_models_comparison.csv\n\n=== FINAL SUMMARY ===\nModel Performance Ranking by Minority F1 (Robust Strategy):\n 1. CNN_Only        -> Minority F1: 0.8457, Accuracy: 0.7645, Overall F1: 0.8457\n 2. LSTM            -> Minority F1: 0.8378, Accuracy: 0.7422, Overall F1: 0.8378\n 3. CNN_BiLSTM      -> Minority F1: 0.8158, Accuracy: 0.7449, Overall F1: 0.8158\n 4. Transformer     -> Minority F1: 0.8117, Accuracy: 0.6831, Overall F1: 0.8117\n 5. Hybrid_CNN_LSTM -> Minority F1: 0.7719, Accuracy: 0.7180, Overall F1: 0.7719\n\nAverage Improvement with Robust Strategy:\n  Baseline average minority F1: 0.8238\n  Robust average minority F1: 0.8166\n  Improvement: -0.0072 (-0.72%)\n=> No overall improvement with robust strategy ❗\n\nSaved artifacts:\n  - Models: ./BASE_DIR/models/robust_models\n  - Analysis CSV: ./BASE_DIR/imbalance_analysis/all_models_comparison.csv\n  - Visualizations: ./BASE_DIR/visualizations/model_comparisons/all_models_comparison.png\n  - Total models trained: 10\n\n=== Pipeline Complete ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x1000 with 4 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import shutil\n\n# Path to the folder you want to zip\nfolder_path = \"/kaggle/working/BASE_DIR\"\n\n# Output zip file path (without .zip extension)\nzip_output = \"/kaggle/working/BASE_DIR\"\n\n# Create the zip archive\nshutil.make_archive(zip_output, 'zip', folder_path)\n\nprint(f\"✅ Zipped successfully: {zip_output}.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T17:31:32.996548Z","iopub.execute_input":"2025-11-01T17:31:32.997305Z","iopub.status.idle":"2025-11-01T17:31:37.024003Z","shell.execute_reply.started":"2025-11-01T17:31:32.997277Z","shell.execute_reply":"2025-11-01T17:31:37.023129Z"}},"outputs":[{"name":"stdout","text":"✅ Zipped successfully: /kaggle/working/BASE_DIR.zip\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Path to CREMA-D\ndata_path = \"/kaggle/input/cremad/AudioWAV\"\nfiles = [f for f in os.listdir(data_path) if f.endswith(\".wav\")]\n\n# Extract emotion codes\nemotions = [f.split('_')[2] for f in files if len(f.split('_')) >= 3]\n\n# Mapping of emotion codes to full names\nemotion_map = {\n    \"ANG\": \"Anger\",\n    \"DIS\": \"Disgust\",\n    \"FEA\": \"Fear\",\n    \"HAP\": \"Happy\",\n    \"NEU\": \"Neutral\",\n    \"SAD\": \"Sad\"\n}\nemotions_full = [emotion_map[e] for e in emotions]\n\n# Define stress vs non-stress grouping\nstress_labels = {\n    \"Anger\": \"Stress\",\n    \"Disgust\": \"Stress\",\n    \"Fear\": \"Stress\",\n    \"Sad\": \"Stress\",\n    \"Happy\": \"Non-Stress\",\n    \"Neutral\": \"Non-Stress\"\n}\n\n# Map to Stress / Non-Stress\nstress_classes = [stress_labels[e] for e in emotions_full]\n\n# Count totals\ncounts = pd.Series(stress_classes).value_counts()\n\n# Compute ratio\nratio = counts[\"Non-Stress\"] / counts[\"Stress\"]\n\nprint(\"=== Stress vs Non-Stress Counts ===\")\nprint(counts)\nprint(f\"\\nRatio: For every 1 Stress sample, there are {ratio:.2f} Non-Stress samples.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T13:33:04.363143Z","iopub.execute_input":"2025-11-01T13:33:04.364036Z","iopub.status.idle":"2025-11-01T13:33:04.387910Z","shell.execute_reply.started":"2025-11-01T13:33:04.363998Z","shell.execute_reply":"2025-11-01T13:33:04.387133Z"}},"outputs":[{"name":"stdout","text":"=== Stress vs Non-Stress Counts ===\nStress        5084\nNon-Stress    2358\nName: count, dtype: int64\n\nRatio: For every 1 Stress sample, there are 0.46 Non-Stress samples.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print('Hellow')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T14:42:34.539395Z","iopub.execute_input":"2025-11-01T14:42:34.540260Z","iopub.status.idle":"2025-11-01T14:42:34.544833Z","shell.execute_reply.started":"2025-11-01T14:42:34.540229Z","shell.execute_reply":"2025-11-01T14:42:34.543875Z"}},"outputs":[{"name":"stdout","text":"Hellow\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}